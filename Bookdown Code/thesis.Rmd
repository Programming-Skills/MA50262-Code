---
title: 'Methods to Accommodate Non-Proportional Hazards in the Analysis of Clinical Trials'
author: 'Martin Adrian Hewing'
date: "`r format(Sys.time(), '%d %B, %Y')`"
institution: 'University of Bath'
division: 'Data Science and Statistics'
advisor: 'Jonathan Bartlett'
# If you have more two advisors, un-silence line 7
#altadvisor: 'Your Other Advisor'
department: 'Mathematics'
degree: 'Master of Science'
knit: bookdown::render_book
site: bookdown::bookdown_site

# This will automatically install the {remotes} package and {thesisdown}
# Change this to FALSE if you'd like to install them manually on your own.
params:
  'Install needed packages for {thesisdown}': True
  
# Remove the hashtag to specify which version of output you would like.
# Can only choose one at a time.
output:
#  thesisdown::thesis_pdf: default 
   thesisdown::thesis_gitbook: default         
#  thesisdown::thesis_word: default
#  thesisdown::thesis_epub: default

# If you are creating a PDF you'll need to write your preliminary content 
# (e.g., abstract, acknowledgements) below or use code similar to line 25-26 
# for the .RMD files. If you are NOT producing a PDF, delete or silence
# lines 25-39 in this YAML header.
abstract: '`r if(knitr:::is_latex_output()) paste(readLines(here::here("prelims", "00-abstract.Rmd")), collapse = "\n  ")`'
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab 
# is needed on the line after the `|`.
acknowledgements: |
  I want to thank a few people.
dedication: |
  You can have a dedication here if you wish. 
preface: |
  This is an example of a thesis setup to use the reed thesis document class 
  (for LaTeX) and the R bookdown package, in general.
  
# Specify the location of the bibliography below
bibliography: bib/thesis.bib
# Download your specific csl file and refer to it in the line below.
csl: csl/apa.csl
lot: true
lof: true
---


<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of 
metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Dedication, for example, simply delete the section entirely, or silence them (add # before each line). 

If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.

If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, include=FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis.
if (!require(remotes)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("remotes", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste('You need to run install.packages("remotes")",
            "first in the Console.')
    )
  }
}
if (!require(thesisdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    remotes::install_github("ismayc/thesisdown")
  } else {
    stop(
      paste(
        "You need to run",
        'remotes::install_github("ismayc/thesisdown")',
        "first in the Console."
      )
    )
  }
}
library(thesisdown)
# Set how wide the R output will go
options(width = 70)
```

<!--
The ackowledgements, preface, dedication, and abstract are added into the PDF
version automatically by inputing them in the YAML at the top of this file.
Alternatively, you can put that content in files like 00--prelim.Rmd and
00-abstract.Rmd like done below.
-->



```{r eval=!knitr::is_latex_output(), child=here::here("prelims", "00--prelim.Rmd")}

```

```{r eval=!knitr::is_latex_output(), child=here::here("prelims", "00-abstract.Rmd")}

```

<!-- The {.unnumbered} option here means that the introduction will be 
"Chapter 0." You can also use {-} for no numbers on chapters.
-->

# The Cox Proportional Hazards model {.unnumbered}
This paper will now proceed to give a concise outline of censoring, the hazard function, the Cox model and some of the most important characteristics of this particular approach.
<br/><br/>
Censoring happens when some information about the survival times of individuals is available, but it is not known precisely. In general there are three separate answers to why censoring is likely to occur. Firstly, the individual does not get the event before the end of the study. Secondly, in the course of the study period, an individual is lost to follow-up, and thirdly, because an individual withdraws from the study, but for a reason other than due to getting the event of interest.
<br/><br/>
Additionally, there are three types of censoring that can happen. Firstly, data that is right-censored. In this case the true time survived is the same as or more than the recorded survival time. Secondly, data that is left-censored. In this case the true time survived is the same as or less than the recorded survival time. Thirdly, data that is interval-censored. In this case the true time survived is within a time interval which is known to the researcher. This project will focus on data that is right censored.
<br/><br/>
The hazard function(also known as the conditional failure rate) is given by: 

<br/><br/>

$$h(t)=\lim _{\Delta t \rightarrow 0} \frac{P(t \leq T<t+\Delta t | T \geq t)}{\Delta t}$$

<br/><br/>

An explanation of the various components of the hazard function are as follows: $\mathit{h(t)}$ is equal to the limit as $\mathit\Delta t$ (gives instantaneous potential) gets closer to zero of the probability that an individual gets the event in the time interval $[t, t+\Delta t]$ given the individual has already survived up to time $\textit{t}$ divided by a small change in $\textit{t}$, noting that, $\mathit{h(t)}$ is not a probability as $\textit{P}$ has been divided by $\mathit\Delta t$. 
<br/><br/>
Formally $\mathit{h(t)}$ can be defined as a function that "gives the instantaneous potential per unit time for the event to occur, given that the individual has survived up to time $\textit{t}$". Alternatively, $\mathit{h(t)}$ can be thought of as like a speedometer. This device gives drivers information on their current velocity, for example a car travelling at 60 miles per hour, has the potential to cover 60 miles in the next hour. However, because the vehicle might accelerate or decelerate in that time period, at that moment when 60 MPH was observed, the speed reading does not show how many miles the car will actually cover, it just gives drivers velocity or instantaneous potential at that moment (this is assuming that it is given the driver has already driven some distance).
<br/><br/>
In a similar fashion the hazard function calculates the potential to get the event being studied at that instant, given survival up to that point in time. Furthermore, it is important to note that for specific $\textit{t}$ time values the hazard function has two properties: firstly, $\mathit{h(t)}$ is not limited to an upper bound value. Secondly, $\mathit{h(t)}$ is never negative, it is more than or equal to zero. 
<br/><br/>
The Cox proportional hazards regression model  formula is given by:

<br/><br/>

$$h(t, \mathbf{X})=h_{0}(t) e^{\sum_{i=1}^{p} \beta_{i} X_{i}}$$

<br/><br/>

$$\mathbf{X}=\left(X_{1}, X_{2}, \dots, X_{p}\right)$$

<br/><br/>

The left hand side of the above expression is the time $\textit{t}$ hazard that an individual faces given a collection of explanatory variables $\mathbf{X}$ whereas, the right hand side of (2) is the product of the baseline hazard function given by $\mathit{{h_{0}(t)}}$ and an exponential expression which is raised to the $\mathit{\beta_{i} X_{i}}$ summed over the $\textit{P}$ predictor $\mathit{X}$ variables. 
<br/><br/>
Additionally in regard to the proportional hazards assumption it is important to note that in the formula given in (2) the baseline hazard is a function of $\textit{t}$ but not the $X^{\prime} s$.  Conversely, the exponential expression does not involve the $\textit{t's}$, but does involve the $X^{\prime} s$. Therefore, the $X^{\prime} s$ are "time-independent" and can be defined as "any variable whose value for a given individual does not change over time". Moreover, two important properties of the Cox proportional hazards model regression model formula are that firstly when all the $X^{\prime} s$ are zero then the formula equals the baseline hazard function as an exponential raised to the zero equals one, and the exponential in the Cox formula ensures that the fitted hazard is not negative. 
<br/><br/>
Secondly, the Cox model is semi-parametric because $\mathit{{h_{0}(t)}}$ the baseline hazard function is unspecified, whereas , parametric models such as the Weibull or Exponential have a fixed functional form, so the Cox model is appropriate in many data situations where functional form of the baseline hazard is unknown as it can often accurately approximate the correct parametric form. 
<br/><br/>
The estimates of the $\beta^{\prime} \mathrm{s}$ (parameters) in the Cox model:  $h(t, \mathbf{X})=h_{0}(t) e^{\sum_{i=1}^{p} \beta_{i} X_{i}}$ are computed using a maximum likelihood approach to give the ML estimates $\hat{\beta}_{i}$. 
<br/><br/>
Another important concept is the hazard ratio. This ratio allows comparison of two individuals that are differentiated by their predictor set ($X^{\prime} s$). Computing the hazard ratio as follows:

<br/><br/>

$$\widehat{H R}=\frac{\hat{h}\left(t, \mathbf{X}^{*}\right)}{\hat{h}(t, \mathbf{X})}$$

<br/><br/>

$$\mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)$$

<br/><br/>

The hazard ratio is the division of $h\left(t, \mathbf{X}^{*}\right)$ by $h(t, \mathbf{X})$. Noting that the individual with the greater hazard is in the numerator (usually the placebo group) so that the hazard ratio is a value greater than one and $\hat{h}\left(t, \mathbf{X}^{*}\right) \geq \hat{h}(t, \mathbf{X})$, which aids in interpretability.

<br/><br/>

$$\widehat{H R}=\frac{\hat{h}_{0}(t) e^{\sum_{e=1}^{p} \hat{\beta}_{i} X_{i}^{*}}}{\hat{h}_{0}(t) e^{\sum_{e^{i=1}}^{p} \hat{\beta}_{i} X_{i}}}=e^{\sum_{i=1}^{p} \hat{\beta}_{i}\left(X_{i}^{*}-X_{i}\right)}$$

<br/><br/>

To arrive at the final formula in (4): Substitute the Cox model formula into the hazard ratio for the respective $\mathbf{X}^{*}$  and $\mathbf{X}$ values to express it in terms of the regression coefficients. Noting that the expression does not depend on $\textit{t}$.

<br/><br/>

The proportional hazards assumption meaning can now be explained, using the hazard ratio.

<br/><br/>

$$\hat{h}\left(t, X^{*}\right)=\hat{\theta} \hat{h}(t, X)$$

<br/><br/>

By letting $\hat{\theta}$ equal to $\exp \left[\sum_{i=1}^{p} \hat{\beta}_{i}\left(X_{i}^{*}-X_{i}\right)\right]$ it follows that $\frac{\hat{h}\left(t, \mathbf{X}^{*}\right)}{\hat{h}(t, \mathbf{X})}=\hat{\theta}$ and so $\hat{h}\left(t, X^{*}\right)=\hat{\theta} \hat{h}(t, X)$ where the constant of proportionality $\hat{\theta}$ is not dependent on $\textit{t}$. Therefore, one subjects hazard function is in proportion to another subjects hazard. 
Klein05

## Diagnostics

Moore D explains that Schoenfeld residuals can be used to assess the proportional hazards assumption:

The partial log-likelihood function is given by: 

<br>
<br>

$$l(\beta)=\sum_{i \in D}\left\{\log \left(\psi_{i}\right)-\log \left(\sum_{k \in R_{i}} \psi_{k}\right)\right\}=\sum_{i \in D}\left\{z_{i} \beta-\log \left(\sum_{k \in R_{i}} e^{z_{k} \beta}\right)\right\}$$

<br>
<br>

The score function is obtained by taking the derivative of the partial log-likelihood function:

<br>
<br>

$$l^{\prime}(\beta)=\sum_{i \in D}\left\{z_{i}-\sum_{k \in R_{i}} z_{k} \cdot p\left(\beta, z_{k}\right)\right\},$$
where
$$p\left(\beta, z_{k}\right)=\frac{e^{z_{k} \beta}}{\sum_{j \in R_{k}} e^{z_{j} \beta}}$$
The Schoenfeld residuals are the score function's individual terms, and every term is calculated by subtracting the expected values* from the observed values of the covariate for the $i^{th}$ patient.

*The expected value is $E\left(Z_{i}\right)=\bar{z}\left(t_{i}\right)$, this is a weighted sum (weight is $p_{k}(\beta)$) of the values of the covariates for at risk patients at that particular time point.

An alternative way to understand the weights is that they are the chance of choosing an indivual from the risk set at time $t$.

Therefore, the residual for $\hat{\beta}$ for the $i^{th}$ event time is:

<br>
<br>

$$\hat{r}_{i}=z_{i}-\sum_{k \in R_{i}} z_{k} \cdot p\left(\hat{\beta}, z_{k}\right)=z_{i}-\bar{z}\left(t_{i}\right)$$
<br>
<br>

When the proportional hazards assumption is not violated a plot of $\hat{r}_{i}$ against the covariate $z_{i}$ will create a collection of points that are centered at zero. Noting that the residuals apply to just the event and not the censoring times. 

<br>
<br>

A further development was put forward by Gramsch et al. in order to make the residuals easier to interpret. The researchers put forward the idea of scaling $\hat{r}_{i}$ by its variance estimate to create a scaled residual: 

<br>
<br>

$$r_{i}^{*}=r_{i} \cdot d \cdot \operatorname{var}(\hat{\beta})$$
where the sum of events is given by $d$ and $\operatorname{var}(\hat{\beta})$ is the parameter estimate variance. 

<br>
<br>

When the hazard ratio is a function of $\beta(t)$ and $t$ it follows that the expectation of the scaled residuals is: 
$$E\left(r_{i}^{*}\right) \approx \beta+\beta(t)$$

<br>
<br>

In this way an approximation of $\beta(t)$ can be obtained by adding the Cox Proportional Hazards estimate $\hat{\beta}$ to the standardised residuals.

<br>
<br>

Finally a Schoenfeld plot of $r_{i}^{*}$ versus $t$ or $log(t)$ can be created to examine the residuals.

<!--chapter:end:index.Rmd-->

<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# Literature Review {#rmd-basics}

Review of the Literature Concerning the Cox Proportional Hazards Regression Model under Conditions of Non-Proportional Hazards, Highlighting Proposed Techniques to Overcome the Violation of the Proportional Hazards Assumption.
The major aim of this project is to assess the robustness the Cox model in regard to non-proportional hazards in an evolving clinical trials environment, and to analyse whether different techniques might be superior in a given situation in terms of efficiency, interpretability or any other factor that could potentially improve the ability to create cost effective life-saving medicines. Furthermore, in regard to objectives, this paper strives to provide robust inferences from analysing simulated data that is interpretable and will potentially advance subject knowledge in this area.
<br/><br/>
This proposal will now outline the processes undertaken to source the relevant literature in regard to Cox regression in the face of non-proportional hazards and methods to overcome these issues. The searches will attempt to locate papers that use methods that have been specifically developed and/or proposed for handling non-proportional hazards in the context of clinical trials. Additionally, examining other survival analysis models that do not have a proportional hazards assumption, and also publications featuring clinical trials that included non-proportional hazards. This information will be utilised in an attempt to achieve the aims and objectives of this paper.
<br/><br/>
The majority of searches carried out in the process of compiling this paper have been carried out via the "Search the literature" section of the Mathematical Sciences homepage on the University of Bath library portal.  In the portal there are a number of relevant databases. However, it was decided to focus on those that specialise in mathematics and/or medicine. 
<br/><br/>
The first platform searched was "PubMed" due to this database specialising in medicine. The criteria of the search was to find research papers with key words in the title or abstract, and this objective was carried out via an advanced search by title and abstract for a number of different in terms sorted by "Best Match" in the PubMed portal starting with the simplest terms to more complex in an attempt to filter out results that are not relevant. 
<br/><br/>
The first term "proportional hazards" returned 106158 results, "non-proportion hazards" returned 54126 results, whereas, "Cox non-proportional hazards" returned 68 results and from this the majority of the papers have been sourced after reading through the abstract, paying attention to citations and trying to find out which researchers might be the most prominent in the field, which techniques and evaluation seemed to make most logical sense in regard to furthering the aims of this project. Searches using the method of experimenting with different combinations of search terms returned. i Google Scholar using "Cox non-proportional hazards " returned 3,980 results, while "alternative Cox non-proportional hazards " returned 2,520, further attempting to refine the search by stipulating a ten year date range further reduced the results returned to 1,710 and from this visually trying to select relevant papers, which is highly problematic from an efficiency point of view. Searches of MathSciNet, Web of Science returned negligible results. The papers that were chosen, have been selected on merit due to the relative impact they have had on this subject area (via critical evaluation in other papers) and the recommendation of this project's supervisor.

## Comparison of Treatment Effects Measured by the Hazard Ratio and by the Ratio of Restricted Mean Survival Times in Oncology Randomized Controlled Trials: Trinquart et al. 2016

Trin16
<br/><br/>
As previously stated a critically important concept within survival analysis is that of censoring. Due to the issues presented by censoring. The consequences are that survival time cannot be viewed as a regular continuous outcome, be summarised using summary statistics such as the mean or standard deviation, visualised in the usual manner using scatterplots etc. or analysed with modelling techniques such as linear regression. However, a major drawback of working in the probability domain is that it can be difficult to interpret and visualise probabilistic relationships such as the hazard ratio. 
<br/><br/>
This gives motivation for the use of restricted mean survival time (RMST). This technique gives practitioners the ability to calculate the mean survival time up to a time point $\textit{t*}$. Formally, for the time-to-event random variable $\textit{T}$, the equation for the true RMST is given by:
<br/><br/>
$$\mu\left(t^{*}\right)=E\left[\min \left(T, t^{*}\right)\right]=\int_{0}^{t^{*}} S(t) \mathrm{d} t$$
<br/><br/>
Where the restricted mean at time $\textit{t*}$ equals the expectation of a minimised function comprising of the random variable $\textit{T}$ and $\textit{t*}$, which is equal to the integral of the survival function at $\textit{t }$ between the limits of 0 and $\textit{t*}$ integrated with respect to $\textit{t}$. An intuitive explanation of this concept is that RMST is equal to the area beneath the survival curve up to a point $\textit{t*}$ and can be thought of as the $\textit{'t* - year life expectancy'}$, for example a recipient of a treatment could be informed that his/her life expectancy with disease A on treatment B over the next 12 months is 6 months or alternatively treatment C increases your life expectancy during the next 24 months by 8 months, when compared to treatment D.Roy11
<br/><br/>
A paper examining the validity of using RMST versus HR is Trinquart $\textit{et al.}$ The authors conducted an empirical study looking at a sample of published phase III "parallel-group" randomised oncology trials from the last six months of 2014 from five separate journals. 
<br/><br/>
The study attempted to ascertain whether the hazard ratio (HR) or the difference of restricted mean survival time (RMST) provided superior results in regard to overcoming some of the drawbacks (incorrect assessments of various treatment effects that are caused by interpreting HR as relative risks and that HR averages are dependent on the duration of follow-up) of using the HR to quantify survival benefit (longer life expectancy and/or improved quality of life). 
<br/><br/>
The method employed by the authors was to reconstruct "individual patient data" (IPD) for each trial of the time to event outcome by comparing HR to RMST. Firstly the researchers conducted a complex search of a number of journals including three general medical ("New England Journal of Medicine", "Lancet", "Journal of the American Medical Association") and two that specialise in oncology ("Journal of Clinical Oncology", "Lancet Oncology"), seeking those journals with reports on non-inferiority and superiority "parallel-group" trials of a randomised nature, whilst discarding all trial types apart from phase III. Secondly, the authors analysed the available trial data from the five journals to extract the non-adjusted HR and 95$\%$ confidence intervals, whilst noting if there was an assessment of any violation of the proportional hazards assumption. From the published Kaplan-Meier survival curves the IPD for each trial was then reconstructed. Furthermore, specialist software was used to compute the time and survival coordinates based on the aforementioned Kaplan-Meier curves. Moreover, the quantity of patients at risk and a total of the number of events was calculated. This information was used to solve inverted Kaplan-Meier equations by using an iterative numerical algorithm. Thirdly, the researchers having estimated the treatment effect in regard to the HR and RMST by the above "IPD" method, proceeded to implement a log-rank test to test for a null treatment effect, additionally using the "Grambsch-Therneau" method to test for the presence of hazards that are non-proportional. A Cox PH model was then used to estimate the HR and variance. If the HR was below a value of one, experimental intervention was favoured. An assessment of the RMST at the time horizon $\textit{t*}$ (pre-specified as the "minimum of the largest observed event time of the two groups") was made in both the control and the experimental group. Furthermore, a determination of the RMST (both the difference and ratio) was calculated from the the survival function via the Kaplan-Meier estimate and the variances were computed using the delta method.
<br/><br/>
The experimental treatment was favoured if the difference in RMST was more than zero, or if a ratio of RMST was more than one. Moreover, a null-treatment effect was tested for using a comparison of the RMST difference (and ratio) with the standard error of the RMST which follows a standard normal distribution. Fourthly, comparing the HR and RMST in regard to the treatment effects, the authors note that the ratio of RMST and HR are two separate methods that quantify the difference in two survival curves, and, therefore, their interpretation is different. However, it is noted that both estimates (which can be thought of as the size of the respective treatment effects) are on an equal relative scale (so they can be compared).
<br/><br/>
The method used to assess the relative differences between HR and RMST was to compute which measure was further from its null systematically. Specifically, two techniques were made use of: i. plotting the HR's versus the ratios and differences of the RMST in the respective trials, furthermore, an  inverse transformation of the HR enabled the researchers to show that if the RMST ratio and/or HR value greater than one would show that the experimental treatment was better. Moreover, using this information it was possible to ascertain the number of occasions that the HR gave a bigger treatment effect than the ratio of RMST in regard to the experimental treatment. ii. A different rule which used the ratio of the RMST to the HR to provide an estimate of differences in treatment effect in each trial: in this rule the HR should be twice as large or below half the RMST ratio. This method provides an estimate of the variance, which was computed via the bootstrap technique. The output (ratio between RMST and HR) can be quantified as a value more that one, shows that the HR approach is superior. 
<br/><br/>
In conclusion, this paper found empirically that RMST conservatively measures the treatment effect versus a measurement via a HR approach. This was the same for overall survival and different time-to-event outcomes, regardless of whether non-proportionality of hazards was present, which relates to the analysis at the start of this section in regard to misinterpretation of HR's and the risk that it poses. Furthermore, difference in RMST gives clinicians the ability to quantify the absolute value of survival differences, which in-turn allows professionals to score the clinical benefit of this effect. Moreover, whether "minimally effective treatment" has been achieved can also be assessed by comparing the treatment under study to the direct clinical benefit threshold which has been predefined beforehand. Additionally, as the researchers found that treatment effects when quantified using HR's and not RMST, regardless of the proportional hazards assumption, appear to be of greater clinical benefit in a systematic way (when in-fact they are not), therefore, the authors recommend that RMST treatment effects measurements should be reported as standard in all trials, where the outcome is "time-to-event". 
<br/><br/>
The researchers also point out that RMST has another advantage over HR's regardless of the proportional hazards assumption in regard to efficiency. When there is a small number of events it causes the HR confidence interval to become wide, whereas, RMST does not suffer from this issue, this would be important in non-inferiority trials. 
<br/><br/>
The authors conclude by discussing the limitations of their study. Importantly, it is recognised that this paper was based on IPD's that were reconstructed, and therefore, some drop in the standard of the IPD's is expected when compared to the original trial IPD's. Furthermore, the integration technique used to estimate the RMST via the Kaplan-Meier curves, is prone to instability when there is a small number of patients at risk. However, this is most likely to happen in the tail of the distribution, and does not often fall in the time horizon that is being measured by RMST. 
<br/><br/>
This proposal will refer to the work of Royston $\textit{et al.}$ RK11 in an attempt to evaluate the research of Trinquart $\textit{et al.}$ Trin16 and how the results of the reviewed paper might impact this project. The main argument in favour of RMST is that this approach takes into account all the survival distribution up to the $\textit{t*}$ value, rather than just a point in time. Furthermore, the RMST is straightforward to interpret and so could have an immediate impact at the clinical level in providing professionals with as much accurate and relevant information when compared to HR's. Moreover, the fact that there is no proportional hazards assumption is an advantage as there is less scope for imprecise results . However, some limitations of RMST have been raised that can also be applied to this research, for example if the $\textit{t*}$ value has been incorrectly specified, then RMST can give results that are not appropriate, calling into question reproducibility by researchers without prior clinical background knowledge. As there is no mention of the risk of misspecification by the authors, it follows that the RMST approach is likely to be advantageous over using HR's in the presence of non-proportional hazards.

## Estimation of treatment effects in weighted log-rank tests Lin et al.

Lin17
<br/><br/>
The analysis of the work of Lin $\textit{et al.}$ will begin with a concise explanation of the log rank test. This procedure is used to test the statistical equivalence of two survival curves for groups of two (or greater). This chi-square test (large sample) gives researchers the ability to get a sense of the overall differences between two survival curves so they can be compared. However, it does not provide evidence that survival curves of the true population are different. For two groups the log-rank test is as follows: 
<br/><br/>
$$H_{0} : \mathbf{S}_{1}(t) = \mathbf{S}_{0}(t)$$
$$H_{1} :  \mathbf{S}_{1}(t) \neq \mathbf{S}_{0}(t)$$
<br/><br/>
$$\textit{For all t, where $\mathbf{S}_{1}(t)$ and $\mathbf{S}_{0}(t)$ are the two true survival functions.}$$
<br/><br/>
To conduct the test a comparison of the Log-rank statistic is made with the chi-square value. If the log-rank statistic is greater than the critical value we reject the null hypothesis and conclude that the survival curves are different.
Klein05
<br/><br/>
This proposal will now proceed to analyse the study conducted by Lin $\textit{et al.}$. Firstly, the authors make note that the log-rank test is often used in the analysis of survival endpoint values. The authors further note that this particular test gives the most statistical power in the presence of proportional hazards when compared to other types of tests. However, under non-proportional hazards the log-rank test suffers from a reduction in statistical power, additionally the regular Cox model gives estimates that are biased. Furthermore, research has found that a general form of the log-rank test known as the weighted log-rank test can lead to an increase in statistical power when non-proportional hazards are present. For example if a large proportion of subjects in a study were to discontinue treatments early, the estimate of the treatment effect is reduced and so there may be a fall in statistical power as those subjects might not continue get any benefit. However, if additional weight is given to earlier time periods in the study when subject discontinuation was less, then the benefit of treatment can be more accurately measured and vice versa. Moreover, the researchers note that it has been shown by Schoenfield Sch81 that if the weights are assigned in a proportional manner to the ratio size of the log hazard this leads to a test with the most possible power. Additionally, when the weights have been pre-selected (based on prior information such as treatment characteristics, study design type and the clinical situation) then type-one errors are preserved. Furthermore, the researchers state that although weighted log-rank tests have been used in past studies, there continues to be issues with how to select an appropriate weight function in regard to the relevant clinical setting and the subsequent interpretation of the results of the test in regard to treatment benefit. 

<br/><br/>

Given the above analysis the authors propose a time varying based Cox model that will provide an estimate of the treatment effect, which will be complementary to the weighted log rank test. Therefore, the weight function  assumptions are explicitly stated in regard to the relative sizes of the treatment benefits through time, and thus are examinable and can be verified in the context of the clinical setting. In addition the score test of the time varying based Cox model is equal to the weighted log-rank test, and so the model estimate gives a value of the treatment effect time profile. The researchers also note that as stated previously the model assumptions can be evaluated in regard to previous knowledge. Furthermore, an assessment of model fit can be made by analysing residual patterns. Moreover, the time profile (treatment effect) estimate can be used to make a judgement on the usefulness of the clinical benefit of the treatments.

<br/><br/>

Now follows a discussion of the technical details of methods undertaken by the authors. The researchers aim is to use their amended Cox model and weighted log-rank test in an attempt to provide evidence that this approach is potentially superior in the presence of non-proportional hazards compared to the standard log-rank test. 

<br/><br/>

A group of $\textit{n}$ subjects are assigned randomly into the control arm or treatment arm of a clinical trial that has an endpoint that is a "time to event". Furthermore, if the i-th subject is randomly assigned to the control arm denoted by $X_{i}=0$ and conversely if assigned to the treatment arm denoted by $X_{1}, X_{2}, \dots X_{n}, X_{i}=1$. Additionally, let the censoring times or event be denoted by $T_{1}, T_{2}, \dots, T_{n}$ and the status be denoted by $\delta_{1}, \delta_{2}, \dots, \delta_{n}$ (censoring is given by $\delta_{i}=0$ and the event is given by $\delta_{i}=1$ respectively). Moreover, let the $\textit{J}$ ordered event times be given by $T_{(1)} \leq T_{(2)} \leq \ldots \leq T_{(J)}$. The weighted log-rank statistic is given by: 
<br/><br/>

$$Z=\frac{\sum_{j=1}^{J} w\left(T_{(j)}\right)\left(O_{j}-E_{j}\right)}{\sqrt{\sum_{j=1}^{J} w\left(T_{(j)}\right)^{2} V_{j}}}$$
<br/><br/>

In the above formula the number of expected ((Noting that the expected are calculated assuming the null hypothesis (of no different in survival curves) is true.)) and observed time $T_{(j)}$ treatment arm events is denoted by $E_{j}$ and $O_{j}$ respectively. The $E_{j}$ variance is given by $V_{j}$ and the non-negative weight function of time is given by $w( .)$. The authors additionally note that there is no change if $w( .)$ is normalised or multiplied by $\textit{k}$ (a constant scalar). 

<br/><br/>

Making use of the above information the authors then state the hazard function for a Cox model (proportional hazards) in regard to the i-th subject as: 
$$\lambda\left(t ; X_{i}\right)=\lambda_{0}(t) e^{w(t) \beta X_{i}}$$

<br/><br/>
The authors explain the treatment coefficient is given by $\beta$ and the baseline hazard function is $\lambda_{0}(t)$. 
They then proceed to show that the weighted log-rank test is equal to the Cox model (10) score test. The approach is further developed as the authors proceed to incorporate the concept of adjustment factor effect into the analysis of the above model, which is explained by the following formula:

<br/><br/>
$$A(t)=\frac{w(t)}{\max (w(t))}$$
<br/><br/>

The researchers then state that the weighted log-rank test, weight function at time $\textit{t}$ is $w(t)$ and $A(t)$ is the adjustment factor weight. As $w( .)$ will produce non-negative values, it follows that $A(t)$ will also be non-negative and will have a maximum value of one at certain point(s) in time. Given the preceding analysis the Cox model hazard function can now be stated as:
<br/><br/>

$$\lambda(t ; X)=\lambda_{0} e^{A(t) \beta X}$$

<br/><br/>

The authors note that there is no change in the $\textit{Z}$ (weighted log-rank statistic) value from scaling by 
$\max (w(t))$ and therefore, the weighted log-rank test that incorporates the weight function $w(t)$ is still equal to the score test of the model given in (9). Furthermore, the hazard function given in (12) can be expressed as a coefficient (constant) that has a covariate which varies with time and can be viewed as:
<br/><br/>

$$X^{*}(t)=A(t) X$$
<br/><br/>

The above formula is a representation of the treatment assignment that has an adjustment factor weighting. Furthermore, an updated Cox model that takes into account the preceding analysis is:

<br/><br/>

$$\lambda(t ; X)=\lambda_{0} e^{\beta X^{*}(t)}$$
<br/><br/>

From this an estimate of the $\beta$ coefficient can be obtained. Additionally, $\hat{\beta}$ estimates from models that have covariates that vary with time have been found to be unbiased in this paper. Furthermore, as $A(t)$ is less than or equal to one, the coefficient $\beta$ is a representation of the maximum effect in the time course. Moreover, the greatest weights (time $\textit{t}$ where $A(t)=1$) are assigned to the points of time where the subjects experience the maximum effect (time $\textit{t}$ where $w(t)=\max (w(t)))$) in the weighted log-rank test that corresponds to this analytical framework. In addition, conditional on using the correct model, the weighted log-rank test (or equally the model score test) gives the highest power and is optimal as per Schoenfield Sch81 which provides the theoretical underpinning for this result. 

<br/><br/>

Proceeding from the above analysis the hazard ratio for the treatment and control arms for a function of time $\mathrm{HR}(t)$ can be derived as follows:

<br/><br/>

$$H R(t)=\frac{\lambda_{0} e^{A(t) \beta \times 1}}{\lambda_{0} e^{A(t) \beta \times 0}}=e^{\beta A(t)}=\left[H R^{F}\right]^{A(t)}$$

<br/><br/>

Where the full effect (maximum effect) is represented by $\mathrm{HR}^{F}=e^{\beta}$ and in the model $e^{\beta A(t)}$ the treatment coefficient $\beta$ is the time varying effect that has the effect treatment factor $A(t)$ as its weight. 

<br/><br/>

In the literature a multitude of different approaches have been put forward in regard to choosing an appropriate weight function each with various strengths and weaknesses. For example, Harrington $\textit{et al.}$ Har82 propose a family of weight function $G^{\rho, \gamma}$ that can give a representation of various different shapes of function based on an observation of survival as follows:
<br/><br/>

$$w(t)=S(t)^{\rho}(1-S(t))^{\gamma}$$
<br/><br/>

Where $S(t)$ is the pooled population survival function; $\gamma$ and $\rho$ are the weight and shape parameters respectively. 
<br/><br/>

When $\gamma$ = $\rho$ = 0, the weighted equals the standard log-rank test and when $\gamma$ = 0 and $\rho$ = 1, the weighted equals the Prentice-Wilcoxon test (a test where the time point $\textit{t}$ weight can be assigned based on time $\textit{t}$ survival). Furthermore, there is more allocation of weight to the middle points of time rather than the two end points when $\gamma$ = 1 and $\rho$ = 1, additionally when $\gamma$ = 1 and $\rho$ = 0 there is more weight allocation to time points that are later. When the $G^{1,1}$ weight function is used there is no prior effect from the treatment effect on the first event, and then effect subsequently rises over the time period, reaching the maximum effect at approximately the median survival time, then the effect falls over time. The following function expresses the preceding analysis mathematically for the choice where rho and gamma are both 1: 
<br/><br/>
$$\mathrm{HR}(t)=\left[\mathrm{HR}^{F}\right]^{S(t)(1-S(t))}$$
<br/><br/>
The researchers make use of simulation studies to test three models. i. Cox model (standard) and log-rank test. ii. A Cox model and weighted log-rank test. iii. A model of short and long term effects proposed by Yang $\textit{et al.}$ Yan10.Using the three approaches/models proposed above, there is consideration of two further settings that considered different scenarios for how the treatment effect changes over time. Firstly; there is a delayed effect of the treatment and secondly; effect of long-term treatment falls when discontinuation of treatment is substantial. Furthermore, a 10,000 run simulation was conducted for various scenarios to calculate a number of different statistical measures such as an estimate of the hazard ratio, statistical power, the standard error and more. In the first setting there is the assumption that there is a minimal effect gained from the treatment at the beginning (HR = 0.9) and then the treatment will impose its maximum effect $\left(\mathrm{HR}^{F}=0.68\right)$ after a delay $\tau_{D}$ of a specific period. In the second setting, the maximum effect of the treatment $\mathrm{HR}^{F}$ is imposed at the start and then falls slowly as more subjects discontinue the treatment. There is a loss of effect as all the subjects have finished their treatments. If $\gamma(t)$ is the proportion of subjects that are still receiving the treatment by time $\textit{t}$, thus the proportion of subjects that have had the treatment discontinued is given by $1- \gamma(t)$. Furthermore, if subjects have a complete loss of the effect of the treatment, that takes place straightaway after stopping the treatment, it follows that the time $\textit{t}$ hazard ratio is given by $\mathrm{HR}(t)=\mathrm{HR}^{F} \gamma(t)+(1-\gamma(t))$. It should be noted that the treatment possibly could have an effect that continues beyond the end of the treatment. The authors found by letting the time period of the additional treatment effect be given by $\tau_{P}$, it follows that $\operatorname{HR}(t)=\operatorname{HR}^{F} \gamma\left(t-\tau_{P}\right)+\left(1-\gamma\left(t-\tau_{P}\right)\right)$ for all $t \geq \tau_{P}$ and $\mathrm{HR}(t)=\mathrm{HR}^{F}$ for all time $t < \tau_{P}$.
<br/><br/>
The results of the simulations in the first setting when the treatment effect was delayed are as follows: In regard to statistical power, the power in the three approaches/models all fall as $\tau_{D}$ rises. However, it is noted that the weighted log-rank test has greater power than the other two tests (score of the regular Cox model and Yang tests) when there is correct specification of the weight function. In terms of estimates of standard error the proposed model provides unbiased estimates, whereas, both the regular model and the Yang tests give biased estimates of the beta due to underestimating the standard error and difficulty separating short/long term effects when $\tau_{D}$ is not equal to zero but close to that value respectively. Furthermore, in general under all three approaches/models there is a preservation of $type-I$ errors with the two Cox models (regular and proposed) performing better than the Yang tests. 
When a Grambschâ€“Therneau test (alpha = $5\%$) is used to test for non-PH it was found that the the regular Cox model violates the PH assumption more than the proposed model and this was exacerbated by longer delay. The explanation is that the the $\textit{A(t)}$ covariate which can vary with time gives the proposed Cox model the flexibility to mitigate this issue. 
Additionally, when a sensitivity analysis was performed, the weighted log-rank test exhibited a loss of power when there was a misspecification in the $\tau_{D}$ value. Although, the power of the weighted log-rank test was still greater than the regular log-rank test. Further analysis points towards robust and stable results even when there is misspecification of $\tau_{D}$ and $A(t)$ in the weighted log-rank test model.
<br/><br/>
In regard to setting two when there was a  long-term reduction in the treatment effect. When $\tau_{P}$ is shorter, there is a fall in the statistical power of both the regular and weighted models, with the proposed model consistently outperforming the regular model. Furthermore, in the proposed model there was less violation of the assumption of proportional hazards and superior statistical performance in relation to the various metrics considered. Whereas, in the regular Cox model was shown to have a bias toward the null hypothesis. In terms of the Yang tests, the performance was below the weighted log-rank test model in regard to statistical power. Moreover, the Yang test had a standard error value that was higher than the other two approaches/models. In addition, in both settings (with and without misspecification), the results are on the whole stable and robust, even though there could be an underestimation of treatment effect.  
<br/><br/>
The authors concede that there are a multitude of different methods available to provide treatment effect estimates in the presence of non-proportional hazards. It would wise to consider the possibility that the Yang test could have chosen from various different techniques (such as adaptive lasso, cubic spines) with prior knowledge that it will not provide superior results to the weighted log-rank test and that other methods could be more suitable. The researchers do acknowledge that methods that flexible might be susceptible to over-fitting and thus perform poorly on unseen data. However, it is unclear why the Yang test was chosen rather than a competing approach, such as the Wilcoxon test. 
<br/><br/>
A clear strength of the authors approach is that they have identified the issue of when the treatment effect is delayed, for example in oncology trials. The approach proposed by the authors in an attempt to overcome these time varying effects clearly has the potential to bring real benefits to patients. On the other hand a weakness of the study is that real data was not used and the researchers relied completely on simulation, and this could have created results that do not reflect real life outcomes in terms of unexpected data structure. 
<br/><br/>

## The use of restricted mean survival time to estimate the treatment effect in randomized clinical trials when the proportional hazards assumption is in doubt Patrick et al. 2011

RK11
<br/><br/>
The authors begin by stating that the hazard ratio (invalid if the proportional hazards assumption is not met) is used in the majority of time-to-event right censored randomised control trials (RCT's), to evaluate whether a regular or prototype treatment is superior. 
<br/><br/>
Many RCT's have a target HR and the authors state that the choice of HR level has been influenced by the seminal papers of Freedman Fre82 and Schoenfield Sch83. In cancer RCT's for example the actual true HR is often around 0.75, between 80$\%$ to 90$\%$ power in a two sided test at a 5$\%$ significance level. A log-rank test is then used to assess the null hypothesis that the HR is equal to one (noting that a likelihood test is a viable alternative due to asymptotic equivalence under the proportional hazards assumption). Furthermore, the log-rank test can be considered somewhat robust to non-PH as it continues to have statistical power even under non-PH when evaluating the control versus the treatment. The log-rank test is used in the comparison of distribution functions, (no assumption of the shape of the distribution is made) while taking censoring into account. The authors further note that at the intersection of survival curves (Kaplan-Meier curves are often used to visualise the control and treatment arm survival curves) there is a loss of statistical power.
<br/><br/>
The researchers proceed to explain that in a time-to-event RCT where a measure of primary outcome is being evaluated, in general two metrics are usually presented; firstly a null hypothesis test and secondly a treatment effect summary. Furthermore, they state it would be prudent to include a test of non-proportional hazards, to backup this statement the authors point towards the work of Mok $\textit{et al.}$ Mok09 as their rationale. This study was criticised for reporting results as significant when non-proportional hazards were present, leading to questions of validity.
<br/><br/>
In an attempt to answer the question to what is an appropriate summary statistic (if one exists) in the presence of non-PH, the authors explore an approach that allows an interpretation of the individual HR as an average of the HR over the duration of follow-up time. They cite a study where a weighted Cox regression estimate was calculated by Schemper $\textit{et al.}$ Sc09 by evaluating a number of average HR's, coming to the conclusion that the average HR (AHR) proposed by Kalbfleisch $\textit{et al.}$ Kal81 was most appropriate.
<br/><br/>
$$\textit{AHR}=\frac{\int\left[h_{1}(t) / h(t)\right] w(t) f(t) \mathrm{d} t}{\int\left[h_{0}(t) / h(t)\right] w(t) f(t) \mathrm{d} t}$$
<br/><br/>
The two group hazard functions are given by $h_{0}(t)$ and $h_{1}(t)$ respectively (Noting that $h(t)=h_{0}(t)+h_{1}(t)$). Additionally $w(t)$ is a user chosen weight function. Furthermore, $f(t)$ is a density function, although there is ambiguity to which distribution it belongs to. 
<br/><br/>
Therefore an alternative approach to providing an appropriate summary statistic in the presence of non-PH could be preferred. The researchers continue to explain that they propose to empirically test an RMST approach used by Zucker Zuc98 as the primary measure when non-PH are present, and the secondary measure when the assumption is satisfied. 
<br/><br/>
The authors go on to state the methodology that they will follow in an attempt to show that the RMST approach is viable versus alternative competing techniques. 
<br/><br/>
Firstly they give a technical explanation of the RMST (Identical to (6) in this proposal), Secondly, they discuss using "pseudo-observations" (the researchers use the term pseudovalues) put forward by Andersen $\textit{et al.}$ And04 in an attempt to examine how covariates affect RMST. The authors then continue to explain that pseudo-values are RMST jackknife estimates which are calculated from the sample survival curve Kaplan-Meier estimate (advantage of providing distribution free RMST estimates), and the entire sample RMST $\textit{t*}$ value estimate is their mean value which is unbiased. Therefore, the authors were able to model the effects of covariates on RMST with the pseudovalues as the response value in a generalised linear model (noting that the standard errors must use the robust "sandwich" estimator). Thirdly, the researchers analyse the Cox model in regard to their RMST approach when the possibility of non-proportional hazards is likely and the regular Cox model would be biased. To overcome the issues with bias they recommend each treatment group (Kaplan-Meier estimate) should be integrated separately. 
<br/><br/>
The survival estimates can then be thought of as coming from a stratified treatment group Cox model. However, the researchers point out that this approach has issues, for example the un-stability of $\widehat{s}_{j}(t)$.
<br/><br/>
Fourthly, the researchers discuss survival models that are parametric and flexible. The authors refer to Royston $\textit{et al.}$ Roy02 as an example of a study that made use of a flexible approach to accommodate various different baseline distributions. The log cumulative hazard function with the covariate vector $\mathbf{x}$ is given by: 
<br/><br/>
$$\ln H(t ; \mathbf{x})=\ln H_{0}(t)+\mathbf{x}^{\prime} \boldsymbol{\beta}=s(\ln t)+\mathbf{x}^{\prime} \boldsymbol{\beta}$$
<br/><br/>
The log cumulative hazard function (baseline) $\ln H_{0}(t)=s(\ln t)$ is computed as a log time cubic spline (restricted), where the spline $s(\ln t)$ is a linear combination of regression parameters $\gamma$ and basis functions given by $s(\ln t)=\gamma_{0}+\gamma_{1} \ln t+\gamma_{2} v_{1}(\ln t)+\cdots+\gamma_{K+1} v_{K}(\ln t)$. The authors explain that all of the $\textit{K}$ + 1 basis functions apart from the initial $\textit{ln t}$ are dependent on an "$\textit{interior knot}$". They continue to state these are pairs of cubic polynomial segments (contiguous) that are joined in log-time and that the basis functions are created so that they are joined at the knots of the polynomial segments. 
<br/><br/>
Some of the advantages of using splines are ease of use in the presence of non-proportional hazards by manipulating the spline function, and as already stated being flexible in regard to different baseline distribution functions.
<br/><br/>
The authors then outline how they estimate the RMST at time $\textbf{x}$ by predicting the cumulative hazard function (log) from (19), then turning it into a survival function and finally integrating that survival function over (0,$\textit{t*}$). Noting that the survival function is totally smooth and specified due to being fully parametric. 
<br/><br/>
Now that the various techniques that the researchers plan to compare have been outlined (i.  Pseudovalues, ii. Stratified treatment group Cox model, iii. Flexible parametric approach), the study proceeds to discuss three selected datasets that have varying degrees of non-proportional hazards. i. An advanced kidney cancer trial (RE01) where no evidence of non-PH has been identified (via a Grambschâ€“Therneau test at the $5\%$ significance level). ii. An advanced ovarian cancer trial (GOG111) with evidence of non-PH via a Grambschâ€“Therneau test with $\textit{P}$ = 0.006. iii. A Lung cancer trial (IPASS) with the presence of non-PH (via significant a Grambschâ€“Therneau test) and crossing survival curves.
<br/><br/>
In trial i, (An advanced kidney cancer trial (RE01)) as there is no evidence of non-proportional hazards the treatment effect is legitimately estimated by the HR. However, the authors highlight the fact that the RMST estimates (flexible parametric and pseudo-values) at $\textit{t*}$ = 4y via the flexible parametric method provide similar results to the Stratified treatment group Cox model and are significant at the 1$\%$ significance level, therefore all the methods give unbiased results as expected.
<br/><br/>
Continuing to trial ii, (advanced ovarian cancer trial (GOG111)) it can be seen that the arm of the experimental treatment shows a marked  improvement in survival. However, because the Grambschâ€“Therneau test of $\textit{P}$ = 0.006 identified the presence of non-PH (if the data censored at year five the test becomes insignificant) it calls into question whether the HR is a legitimate measure of survival when estimated via the Stratified treatment group Cox model. The researchers state that the RMST estimates at $\textit{t*}$ = 7y could give reliable estimates even if non-PH are a concern.  
<br/><br/>
In trial iii, (A Lung cancer trial (IPASS)) the authors state that crossing survival curves (progression free survival) point towards the presence of highly significant levels of non-PH, and therefore cast doubt on the validity of using the Stratified treatment group Cox model in this situation. However, both the RMST approaches give significant results and would be preferred over methods that are susceptible to non-PH. Noting that the authors do discuss the possibility over both overfitting and under-fitting the data in regard to RMST in the respective trials, surmising that overfitting is usually not as much as a problem as under-fitting (it may bias the restricted mean), and that adjusting the degrees of freedom in the flexible model is the best solution to this issue.
<br/><br/>
The above analysis is then used to come to a reasoned conclusion about how to design future clinical trials so that they can potentially be robust to the influence of non-PH. The researchers, outline a four stage plan that could be followed. Firstly, use a log-rank test (somewhat robust to non-PH) to carry out an appropriate hypothesis test on treatment effect and come to reasoned conclusions. Secondly, irrespective of the significance of the log-rank test, a Grambschâ€“Therneau test (Scaled Schoenfeld residual approach) should be carried out to ascertain if non-PH are present (visualisations can help).
Thirdly, when non-PH are not found, the HR is the most relevant treatment effect primary summary (including confidence interval). Fourthly, if non-PH have been detected then the most appropriate estimate of the treatment effect primary measure is the $\textit{t*}$ difference in RMST (and CI) via either the pseudovalue or flexible parametric technique. Noting that in most cases the most sensible $\text{t*}$ value is motivated by clinical requirements.
<br/><br/>
In terms of strengths and weaknesses of the preceding recommendations in regard to using RMST, the authors do comment that some of the major advantages of using this approach are the interpretability, lack of PH assumption (i.e gives robust accurate results in various scenario's). However, limitations are that the $\textit{t*}$ value must be carefully chosen so not to lead to results that are misleading. Therefore, clinical expertise must be applied to the choice of $\textit{t*}$. 
<br/><br/>
In addition to the authors evaluation, the opinion of this proposal in regard to strengths of the above paper are that they used two different RMST approaches which allowed the reader to compare the results of both  Pseudovalues and Flexible parametric to the standard Cox model approach,  and this gave more confidence in their findings as both performed well with non-PH. Furthermore, they used real data in their analysis which could help highlight the need for RMST to be used more often (simulation studies can feel somewhat abstract, but increases in survival from real world data are potentially more believable somehow). In terms of limitations, it might be difficult to tune the models that the authors have proposed without specialist knowledge of the clinical treatment area, leading to issues of reproducing their findings with another dataset.   

<!--chapter:end:01-chap1.Rmd-->

# Reconstructing Individual Patient Data  {#math-sci}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

There has been an increasing need for researchers within academia and/or the pharmaceutical industry to gain access to individual clinical trial patient data (IPD) so that they can perform various secondary analyses. However, due to the often-large sums that drug companies invest into research and development, these firms are understandably reluctant to grant free access to their data until they have fully made use of this resource. Therefore, there is the need to try to recreate IPD from published clinical trial reports through various methods. 

As a postgraduate student on an industrial placement this project had originally sought to make use of study data applied for under the placement companyâ€™s data sharing scheme in order to investigate different methods for handling non-proportional hazards. However, this request was rejected on the grounds that the data was still business sensitive. 

Through various industrial and academic sources, researchers are often able to access published Kaplan Meier (KM) curves and their corresponding summary statistics (CI and p-values), the numbers at risk in the study and potentially the total number of events witnessed in the clinical trial. 

From the published KM curves and the patient at risk data Wan et al. 2015 identified four methods that are capable of re-creating Individual Patient Data (IPD). 

## The Least Squares approach

This method can estimate the parameters via the minimisation of the sum of the squared residuals, where the residuals are the difference between the actual and estimated data. 

For example if 30 points of data $\left(\mathrm{x}_{1}, \mathrm{y}_{1}\right),\left(\mathrm{x}_{2}, \mathrm{y}_{2}\right), \ldots,\left(\mathrm{x}_{30}, \mathrm{y}_{30}\right)$ have been extracted from a KM curve, and are following a distribution that has a function in the form $\mathrm{y}=\mathrm{f}(\mathrm{x}, \beta)$ where $\beta=(\beta 1, \beta 2)$. The parameter $\beta$ vectors could be estimated via the method of Least Squares, in which the sum of the squared residuals has been minimised.

## Graphical Approach

A parametric model can be estimated via the graphical method. This approach requires the researcher to make a transformation of the survival function to a function with a linear form. A linear line is then fitted to the collection of points that have been extracted from the published KM curve. 

For example, the survival function of the Weibull distribution is: $$S(t)=e^{-\lambda t^{\gamma}}$$ Where $\gamma$ is the shape and $\lambda$ is the scale parameter respectively. 

A transformation of this distribution can be made by taking the logarithm of the preceding equation: $$\log (S(t))=-\lambda t_{\gamma}$$ 

Taking the logarithm for the second time gives: $$\log (-\log (S(t)))=\log (\lambda)+ \gamma *\log (t)$$
From this formula it is possible to plot $$\log (-\log (S(t)))\ versus\ \log (t)$$ and henceforth make an estimate of the parameters. 

## Interval-censored data estimation approach

Hoyle et al. put forward an approach to estimate IPD using the total number of patients, the respective numbers at risk, and the corresponding survival probabilities. However, this method assumes that there is constant censoring in each of the time intervals. Furthermore, the number of censorships and events in every time interval of a quarter length were estimated, which lead to a large improvement of the fit of the curves. Moreover, when information on patients at risk is not known, an estimate of the number of censorship and events can be made using the approach put forward by Tierney et al. 2007. Additionally, the authors note that the method will become more accurate if more intervals are used and that the estimated IPD will be interval censored. 

The researchers compared their approach to the two traditional methods mentioned above (Least Squares and the Graphical approach) using a Monte Carlo simulation. The elements included in the simulation were the various combinations of parameters in the Weibull distribution, the sample size and the type of censoring. The authors proved that their approach was superior to the traditional method with respect to both fitting the curve and the estimation of the mean survival time. Importantly, the estimate of the mean survival time using the Hoyle et al. approach was as accurate as an estimate that was obtained by calculating the maximum likelihood estimator (MLE) on the actual IPD. Additionally, it is worth noting that another benefit over the traditional methods is that this method can capture uncertainty. Several drawbacks, of the authors research are that they did not vary their censoring proportions and only the Weibull distribution was considered in their Monte Carlo simulation.

## Precise survival data approach

An approach developed by Guyot et al. 2012 that can be used with data that is left or right censored, makes use of an iterative algorithm. From this recreated IPD the survival curves can be reconstructed. In constrast to the approach put forward by Hoyle et al. the estimated IPD is precise survival data and not interval survival data. However, an assumption of Guyot et al. is that the rate of censoring is constant within a given time interval. 

A concise explaination of the Guyot et al. algorithm is now given: 

```{r, echo=FALSE, fig.cap="Guyot Algorithm Flowchart"}
knitr::include_graphics(path = "figure/guyot.flowchart.PNG")
```

The intitial estmate of the number of censored subjects is obtained for the $i^{th}$ interval. From this ititial estimate, a calculation of the number of censored subjects that occured between the extracted KM co-ordinate $k$ and $k+1$ is made. Now this calculation is used to estimate the number of events that occur at every extracted KM co-ordinate $k$ and also the numbers of patients at risk at $k+1$. This information is used to compare the estimate of the number of patients at risk at the start of the interval $i$ to the reported number at risk at the beginning of the interval $i$, and if the numbers are not equal the process repeats itself until the two numbers are equal and this matches the previous iteration. Conversely, in the situation where, the total number of the reported events is greater than the estimate of the number of events the process repeats itself until the two numbers equal from the previous iteration.

Guyot et al. evaluated the accuracy of their approach by comparing the findings to the reported statistics in four papers that they selected to recreate, choosing not to carry any simulations. These included 22 probabilites of survival, 7 times of median survival, 6 hazard ratios and 4 log hazard ratio standard errors. The researchers discovered that their approach was highly accurate for both median survival times and survival probabilities and the accuracy was acceptable when the total number of events or the number of patients at risk was given. 

## Evaluation of the Four Approaches

Wan et al. 2015 compared all four approaches on simulated data and found that the least squares and graphical methods performed badly due to their tendency to over-estimate by a large margin in certain situations. Furthermore, when the Weibull distribution was used both the Hoyle et al. and Guyot et al. methods gave good estimates and uncertainty values of the mean survival time. In-fact the accuracy of the estimated values was as accurate as those obtained using the actual IPD. However, when the lognormal distribution was used ($\sigma=2, \mu=0.303$ the hazard function falls over almost all values of time) the Guyot et al. approach was markedly more accurate (less MLE bias) when the rate of censoring was high 42% and 76% respectively, when compared to the Hoyle et al. approach (Shen et al. also found that the MLE approach was biased in the presence of heavily censored data). Moreover, Wu et al. noted that in many studies there is a high censoring rate, they estimate this to be as high as 70% in many clinical trials. Additionally, Tai et al. state that the lognormal distribution is much more widely used in fitting cancer site data. Therefore, this project concludes that in a situation where, there is a high level of censoring and the lognormal distribution is much more likely to be used it seems prudent to make use of the Guyot et al. approach over alternative methods.

## Implementation of the Guyot et al. Approach

To implement the Guyot et al. algorithm and reconstruct the IPD for a published KM curve a number of steps must be undertaken. Noting that it is imperative that the number of patients at risk and/or the total number of events must be known to apply this method.

Several reseachers have made use of the Guyot et al. algorithm in their work, this project will be using the method used by Satagopan et al. and some adjustments proposed by Magirr D and Biao et al. .

The first step was to source the studies that exhibit non-proportional hazards. (This project decided to source two studies for each type of non-proportional hazards, including, crossing hazards, a delayed treatment effect or diminishing treatment effect). 

```{r, echo=FALSE, fig.cap="Various Types of Non-Proportional Hazards"}
knitr::include_graphics(path = "figure/np.PNG")
```

The main source of research papers that feature non-proportional hazards have been via the techniques used in the literature review. This has involved a continued search via the Mathematical Sciences hub on the University of Bath library portal, various databases including "Pubmed" using a modified keyword search to locate studies that exhibit non-proportional hazards for example "Oncology non-proportional hazards" returned 878 results, while "Clinical Trial non-proportional hazards" returned 943 results. Furthermore, as this is a contemporary issue which is having a significant impact on clinical trials, especially those focusing on oncology and immunotherapy many pharmacuetical companies are now working together to discuss these challenges and propose solutions. The information from these working groups are often then freely distributed. An example of one such working group is: The public workshop hosted at the Duke, Robert J. Margolis, MD Center for Health Policy titled "Oncology Clinical Trials in the Presence of Non-Proportional Hazards". This session examined various studies that featured non-proportion hazards and how different approaches might be used to overcome the problem of the violation of non-proportional hazards, and a follow up research paper created by Lin et al. 2020 titled: "Alternative Analysis Methods for Time to Event Endpoints Under Nonproportional Hazards: A Comparative Analysis" (discussed in the literature review). 

Once the relevant studies featuring non-proportional hazards have been sourced, as per the method undertaken by Satagopan et al. the first stage is to load the png image into Adobe Illustrator and to convert the image into a high fidelity photo. This enables the separate KM curves to be isolated from each other and saved to separate jpeg files. Then the separate images can be loaded into a software program such as "DigitalizeIT" that will facilitate the digital extraction of the survival and time probabilities. In this software program the researcher must stipulate the minimum and maximum X and Y values respectively and trace the outline of the KM curve either automatically or via a manual approach which involves tracing the curve with multiple mouse clicks. This data is then introduced to the algorithm after the researcher has stipulated which time ranges to consider. Noting that each treatment arm must be considered separately The output result will be IPD for each treatment arm which the researcher can bind together to create IPD for each study. This was carried out for the six studies that were selected for this project. 

It is worth noting that an issue faced in this process was that the researcher must ensure that their data is monotonically decreasing and well defined. Furthermore, Biao et al. explains that if the researcher stipulates the number at risk parameter after the automatic collection of data points (the software can set too many instances of the same survival (y-axis) to different time points (x-axis)) there is the risk of upsetting the delicate balance between the amount of censored that are needed to ensure proportionality and overall numbers (of censored) between time points. If this relationship is broken then the vector of censored will contain negative numbers and break the algorithm. This problem occured during the creating of IPD for this project. However, a solution was to create a custom function that would remove values that where non-monotonically decreasing and to use the manual tracing method for the collection of some data-sets. 

```{r, echo=FALSE, fig.cap="Red points show Non-Monotonically Decreasing Data"}
knitr::include_graphics(path = "figure/mono3.PNG")
```

<br>
<br>

The Reconstructed vursus the Published Kaplan Meier Curves placed side by side for comparson.

```{r, echo=FALSE, fig.cap="Mok et al. 2009 Figure A - Reconstructed vs Published Kaplan Meier Curves and Patients at Risk"}
knitr::include_graphics(path = "figure/mok.combined.svg")
```

## Math

\TeX\ is the best way to typeset mathematics. Donald Knuth designed \TeX\ when he got frustrated at how long it was taking the typesetters to finish his book, which contained a lot of mathematics.  One nice feature of _R Markdown_ is its ability to read LaTeX code directly.

If you are doing a thesis that will involve lots of math, you will want to read the following section which has been commented out. If you're not going to use math, skip over or delete this next commented section.


<!-- MATH and PHYSICS majors: Uncomment the following section -->
<!--
$$\sum_{j=1}^n (\delta\theta_j)^2 \leq {{\beta_i^2}\over{\delta_i^2 + \rho_i^2}}
\left[ 2\rho_i^2 + {\delta_i^2\beta_i^2\over{\delta_i^2 + \rho_i^2}} \right] \equiv \omega_i^2
$$

From Informational Dynamics, we have the following (Dave Braden):

After _n_ such encounters the posterior density for $\theta$ is

$$
\pi(\theta|X_1< y_1,\dots,X_n<y_n) \varpropto \pi(\theta) \prod_{i=1}^n\int_{-\infty}^{y_i}
   \exp\left(-{(x-\theta)^2\over{2\sigma^2}}\right)\ dx
$$

Another equation:

$$\det\left|\,\begin{matrix}%
c_0&c_1\hfill&c_2\hfill&\ldots&c_n\hfill\cr
c_1&c_2\hfill&c_3\hfill&\ldots&c_{n+1}\hfill\cr
c_2&c_3\hfill&c_4\hfill&\ldots&c_{n+2}\hfill\cr
\,\vdots\hfill&\,\vdots\hfill&
  \,\vdots\hfill&&\,\vdots\hfill\cr
c_n&c_{n+1}\hfill&c_{n+2}\hfill&\ldots&c_{2n}\hfill\cr
\end{matrix}\right|>0$$


Lapidus and Pindar, Numerical Solution of Partial Differential Equations in Science and
Engineering.  Page 54

$$
\int_t\left\{\sum_{j=1}^3 T_j \left({d\phi_j\over dt}+k\phi_j\right)-kT_e\right\}w_i(t)\ dt=0,
   \qquad\quad i=1,2,3.
$$

L\&P  Galerkin method weighting functions.  Page 55

$$
\sum_{j=1}^3 T_j\int_0^1\left\{{d\phi_j\over dt} + k\phi_j\right\} \phi_i\ dt
   = \int_{0}^1k\,T_e\phi_idt, \qquad i=1,2,3 $$

Another L\&P (p145)

$$
\int_{-1}^1\!\int_{-1}^1\!\int_{-1}^1 f\big(\xi,\eta,\zeta\big)
   = \sum_{k=1}^n\sum_{j=1}^n\sum_{i=1}^n w_i w_j w_k f\big( \xi,\eta,\zeta\big).
$$

Another L\&P (p126)

$$
\int_{A_e} (\,\cdot\,) dx dy = \int_{-1}^1\!\int_{-1}^1 (\,\cdot\,) \det[J] d\xi d\eta.
$$
-->

## Chemistry 101: Symbols

Chemical formulas will look best if they are not italicized. Get around math mode's automatic italicizing in LaTeX by using the argument `$\mathrm{formula here}$`, with your formula inside the curly brackets.  (Notice the use of the backticks here which enclose text that acts as code.)

So, $\mathrm{Fe_2^{2+}Cr_2O_4}$ is written `$\mathrm{Fe_2^{2+}Cr_2O_4}$`.

<!--
The \noindent command below does what you'd expect:  it forces the current line/paragraph to not indent. This was done here to match the format of the LaTeX thesis PDF.
-->

\noindent Exponent or Superscript: $\mathrm{O^-}$

\noindent Subscript: $\mathrm{CH_4}$

To stack numbers or letters as in $\mathrm{Fe_2^{2+}}$, the subscript is defined first, and then the superscript is defined.

\noindent Bullet: CuCl $\bullet$ $\mathrm{7H_{2}O}$


\noindent Delta: $\Delta$

\noindent Reaction Arrows: $\longrightarrow$ or  $\xrightarrow{solution}$

\noindent Resonance Arrows: $\leftrightarrow$

\noindent Reversible Reaction Arrows: $\rightleftharpoons$

### Typesetting reactions

You may wish to put your reaction in an equation environment, which means that LaTeX will place the reaction where it fits and will number the equations for you. 

\begin{equation}
  \mathrm{C_6H_{12}O_6  + 6O_2} \longrightarrow \mathrm{6CO_2 + 6H_2O}
  (\#eq:reaction)
\end{equation}

We can reference this combustion of glucose reaction via Equation \@ref(eq:reaction).

### Other examples of reactions

$\mathrm{NH_4Cl_{(s)}}$ $\rightleftharpoons$ $\mathrm{NH_{3(g)}+HCl_{(g)}}$

\noindent $\mathrm{MeCH_2Br + Mg}$ $\xrightarrow[below]{above}$ $\mathrm{MeCH_2\bullet Mg \bullet Br}$

## Physics

Many of the symbols you will need can be found on the math page <https://web.reed.edu/cis/help/latex/math.html> and the Comprehensive LaTeX Symbol Guide (<https://mirror.utexas.edu/ctan/info/symbols/comprehensive/symbols-letter.pdf>).

## Biology

You will probably find the resources at <https://www.lecb.ncifcrf.gov/~toms/latex.html> helpful, particularly the links to bsts for various journals. You may also be interested in TeXShade for nucleotide typesetting (<https://homepages.uni-tuebingen.de/beitz/txe.html>).  Be sure to read the proceeding chapter on graphics and tables.


<!--chapter:end:02-chap2.Rmd-->

```{r include_packages_2, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if (!require(remotes)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("remotes", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("remotes")',
        "first in the Console."
      )
    )
  }
}
if (!require(dplyr)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("dplyr", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("dplyr")',
        "first in the Console."
      )
    )
  }
}
if (!require(ggplot2)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("ggplot2", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("ggplot2")',
        "first in the Console."
      )
    )
  }
}
if (!require(bookdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("bookdown", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("bookdown")',
        "first in the Console."
      )
    )
  }
}
if (!require(thesisdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    remotes::install_github("ismayc/thesisdown")
  } else {
    stop(
      paste(
        "You need to run",
        'remotes::install_github("ismayc/thesisdown")',
        "first in the Console."
      )
    )
  }
}
library(thesisdown)
library(dplyr)
library(ggplot2)
library(knitr)
flights <- read.csv("data/flights.csv", stringsAsFactors = FALSE)
```

# Analysis of Individual patient Data {#ref-labels}

<br>
<br>

## Mok et al. 2009 IPASS Study: Crossing Hazards

<br>
<br>

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Overall Study Results"}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

km_trt_fit <- survfit(Surv(time, event) ~ arm, data = IPD.Mok.A)

ggsurv <- ggsurvplot(
  km_trt_fit,                     # survfit object with calculated statistics.
  data = IPD.Mok.A,             # data used to fit survival curves.
  risk.table = TRUE,       # show risk table.
  pval = FALSE,             # show p-value of log-rank test.
  conf.int = FALSE,         # show confidence intervals for 
  # point estimates of survival curves.
  palette = c("#E7B800", "#2E9FDF"),
  xlim = c(0,20),         # present narrower X axis, but not affect
  # survival estimates.
  xlab = "Months since Randomization",   # customize X axis label.
  ylab = "Pr of Progression-free Survival",   # customize X axis label.
  break.time.by = 4,     # break X axis in time intervals by 500.
  ggtheme = theme_light(), # customize plot and risk table with a theme.
  risk.table.y.text.col = T,# colour risk table text annotations.
  risk.table.height = 0.25, # the height of the risk table
  risk.table.y.text = FALSE,# show bars instead of names in text annotations
  # in legend of risk table.
  ncensor.plot = FALSE,      # plot the number of censored subjects at time t
  ncensor.plot.height = 0.25,
  conf.int.style = "step",  # customize style of confidence intervals
  surv.median.line = "hv",  # add the median survival pointer.
  legend.labs = c("Carboplatin", "Gefitinib")    # change legend labels.
)

# Labels for Survival Curves (plot)
ggsurv$plot <- ggsurv$plot + labs(
  title    = "Kaplan-Meier Curves for Progression-free Survival",                  
  subtitle = "Overall Study Results"
)

# Changing the font size, style and color

ggsurv <- ggpar(
  ggsurv,
  font.title    = c(16, "bold", "black"),         
  font.subtitle = c(10, "bold.italic", "black"), 
  font.caption  = c(14, "plain", "black"),        
  font.x        = c(14, "bold.italic", "black"),          
  font.y        = c(14, "bold.italic", "black"),      
  font.xtickslab = c(12, "plain", "black"),
  legend = "top"
)

ggsurv
```

<br>
<br>

The IPASS study which was titled: "Gefitinib or Carboplatinâ€“Paclitaxel in Pulmonary Adenocarcinoma" was a phase 3 clinical trial (open label).

Previously untreated East Asian patients who were suffering from nonâ€“small-cell lung cancer (advanced pulmonary adenocarcinoma) and were either former light smokers or non-smokers were assigned randomly to be treated with either a daily dose of 250mg of gefitinib (609 subjects) or a dose of carboplatin of 5 to 6mg per mililiter per minute and a dose a 200mg per meter squared of body-surface area (608 subjects). 

The primary endpoint of the trial was the progression free survival of the subject. 

From a visual inspection it is apparent that the experimental treatment begins with a greater hazard but there is a crossing of the treatment arms at around six months after that time the control arm presents the higher hazard.

<br>
<br>

```{r, echo=FALSE, fig.cap= "Schoenfeld Plot for Mok et al. 2009 IPASS Study", warning=FALSE, message=FALSE}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

colnames(IPD.Mok.A) <- c("time", "event", "Gefitinib")

# fit coxph model
cox.model <- coxph(Surv(time, event) ~ Gefitinib, data = IPD.Mok.A) 

# Schoenfeld plot
par(mfrow=c(1,1))
plot(cox.zph(cox.model, transform = "km"), main = "Schoenfeld Individual Test p: 2e-16")
abline(h=0, col=2)

# cox.zph(cox.model)
```
The Schoenfeld residual plot is an effective method to assess the proportional hazards assumption when combined with a visual inspection of the Kaplan Meier curves and a global test of non-proportional hazards. 

<br>
<br>

```{r echo=FALSE, fig.cap="Cox Proportional Hazards Model"}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

colnames(IPD.Mok.A) <- c("time", "event", "arm")

# fit coxph model
cox.model <- coxph(Surv(time, event) ~ arm, data = IPD.Mok.A) 

# format results into data frame with global p-values
cox.model %>%
  tbl_regression(
    show_single_row = arm,
    label = arm ~ "Gefitinib vs Carboplatin-Paclitaxel",
    exponentiate = TRUE) %>%
  bold_p(t = 0.10) %>%
  bold_labels() %>%
  italicize_levels()
```

<br>
<br>

----------------------------------------------------------------------------------
  Average HR                           95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  0.965                              0.844, 1.104                   0.605   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher) Weighted Cox Proportional Hazards Model

At a given instant in time a patient recieving the Gefitinib treatment is 0.965 times as likely to die as a patient who is on the Carboplatin-Paclitaxel treatment. The 95% confidence interval was (0.844, 1.104) with p value 0.605.

<br>
<br>

----------------------------------------------------------------------------------
  $\hat{c}^{\prime} \%$                95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  49.12%                             0.458, 0.525                   0.605   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher1) Weighted Cox Proportional Hazards Model (Generalized Concordance)

With a probability of 49.12% (95% CI:45.8%, 52.5%), a patient treated with Gefitinib expected (at the baseline value) to die earlier than a patient treated with Carboplatin-Paclitaxel with p value 0.605.

<br>
<br>

```{r, echo=FALSE, fig.cap="Weights used by weighted Cox regression are plotted against time"}
knitr::include_graphics(path = "figure/mok.a.coxphw.svg")
```
In this study the increase in the censoring weights are small during the first six months of treatment, after this they begin to rise rapidly. However, it should be noted that the normalised total weight range is [0,25, 2]. 

```{r, include=FALSE}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

# Pooled Survival Function S(t-) or 1-S(t-) = S(t)
S <- survfit(Surv(time, event) ~ 1, data = IPD.Mok.A)
```

<br>
<br>

```{r, echo=FALSE}
# FH(1,0)  1-S(t-) vs t (Prentice-Wilcoxon)
plot(x = S$time, y = S$surv,
     main = "FH(1,0)  1-S(t-) vs Time (Prentice-Wilcoxon)", 
     xlab = "Time",
     ylab = "1-S(t-)",
     col = "blue")
```

<br>
<br>

```{r, echo=FALSE}
# FH(0,1) S(t) against t
plot(x = S$time, y = (1-S$surv),
     main = "FH(0,1) S(t) vs Time", 
     xlab = "Time",
     ylab = "S(t)",
     col = "blue")
```

<br>
<br>

```{r, echo=FALSE}
# Log.Rank 
plot(x = S$time, 
     y = rep(1, times = length(S$time)),
     main = "FH(0,0) Constant Function One Vs Time", 
     xlab = "Time",
     ylab = "Constant Function 1",
     col = "blue")
```

<br>
<br>

```{r, echo=FALSE}
# FH(1,1) S(t)*(1-S(t))
plot(x = S$time, 
     y = (S$surv)*(1-(S$surv)), 
     main = "FH(1,1) S(t)*(1-S(t))", 
     xlab = "Time",
     ylab = "S(t)*(1-S(t)",
     col = "blue")
```

<br>
<br>

```{r, echo=FALSE, fig.cap="Weighted Log Rank Test"}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

colnames(IPD.Mok.A) <- c("time", "event", "arm")

IPD.Mok.A$event <- ifelse(IPD.Mok.A$event == 0, 1, 0)

result <- wlr.Stat(surv=IPD.Mok.A$time, cnsr=IPD.Mok.A$event, trt= IPD.Mok.A$arm, fparam=list(rho=c(0,0,1,1), gamma=c(0,1,1,0), wlr='FH(0,1)', APPLE=NULL))

result %>% 
  kableExtra::kable(caption = "Weighted Log Rank Test")
```

<br>
<br>

```{r, echo=FALSE, include=FALSE}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

rgs <- list(c(0, 0), c(0, 1), c(1, 1), c(1, 0))

draws <- 10

IPD.Mok.A$arm <- ifelse(IPD.Mok.A$arm == 0, "control", "experimental")

IPD.Mok.A$event <- ifelse(IPD.Mok.A$event == 0, 1, 0)

result.mc <- nphsim::combo.wlr(survival = IPD.Mok.A$time, cnsr = IPD.Mok.A$event, trt = IPD.Mok.A$arm, fparam = list(rgs=rgs,draws=draws))

result.mc <- lapply(result.mc, function(x) round(x, digits = 3))
```

```{r, echo=FALSE}
as.data.frame(result.mc) %>% kableExtra::kable(caption = "Max Combo Test")
```

<br>
<br>

```{r, include=FALSE, message=FALSE}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

IPD.Mok.A$arm <- ifelse(IPD.Mok.A$arm == 0, "control", "experimental")

IPD.Mok.A$event <- ifelse(IPD.Mok.A$event == 0, 1, 0)

result.rmst <- rmst.Stat(survival = IPD.Mok.A$time, cnsr = IPD.Mok.A$event, trt = IPD.Mok.A$arm, stra = NULL, fparam = 20)

result.rmst_ <- lapply(result.rmst, function(x) round(x, digits = 5))
```

```{r, echo=FALSE}
as.data.frame(result.rmst_) %>% kableExtra::kable(caption = "Restricted Mean Survival Time")
```

```{r, include=FALSE}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

result.rmst2 = rmst2(time = IPD.Mok.A$time, status = IPD.Mok.A$event, arm = IPD.Mok.A$arm, tau = 20)

result.rmst2
```

The point estimate indicated that patients on the Gefitinib treatment survive `r round(result.rmst2$unadjusted.result[1,1],1)` months longer than those on Carboplatin-Paclitaxel group on average, when following up the patients `r result.rmst2$tau` months. The 95% confidence interval was (`r round(result.rmst2$unadjusted.result[1,2],1)` to `r round(result.rmst2$unadjusted.result[1,3],1)`) with p value `r round(result.rmst2$unadjusted.result[1,4],2)`.

```{r, echo=FALSE}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

result.rmst2 = rmst2(time = IPD.Mok.A$time, status = IPD.Mok.A$event, arm = IPD.Mok.A$arm, tau = 20)

plot(
result.rmst2,
xlab = "Months",
ylab = "Probability",
col = "black",
col.RMST = "#E7B800", 
col.RMTL = "#2E9FDF",
density = 80,
angle = 85
)

```

<br>
<br>

\clearpage 

## Brehmer et al. 2016 Nivolumab versus Docetaxel Study: Crossing Hazards

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Kaplanâ€“Meier curves for overall survival"}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

km_trt_fit <- survfit(Surv(time, event) ~ arm, data = IPD.Brahmer.a)

ggsurv <- ggsurvplot(
  km_trt_fit,                     # survfit object with calculated statistics.
  data = IPD.Brahmer.a,             # data used to fit survival curves.
  risk.table = TRUE,       # show risk table.
  pval = FALSE,             # show p-value of log-rank test.
  conf.int = FALSE,         # show confidence intervals for 
  # point estimates of survival curves.
  palette = c("#E7B800", "#2E9FDF"),
  xlim = c(0,25),         # present narrower X axis, but not affect
  # survival estimates.
  xlab = "Months",   # customize X axis label.
  ylab = "Overall Survival (% of patients)",   # customize X axis label.
  break.time.by = 3,     # break X axis in time intervals by 500.
  ggtheme = theme_light(), # customize plot and risk table with a theme.
  risk.table.y.text.col = T,# colour risk table text annotations.
  risk.table.height = 0.25, # the height of the risk table
  risk.table.y.text = FALSE,# show bars instead of names in text annotations
  # in legend of risk table.
  ncensor.plot = FALSE,      # plot the number of censored subjects at time t
  ncensor.plot.height = 0.25,
  conf.int.style = "step",  # customize style of confidence intervals
  surv.median.line = "hv",  # add the median survival pointer.
  legend.labs = c("Docetaxel","Nivolumab")    # change legend labels.
)

# Labels for Survival Curves (plot)
ggsurv$plot <- ggsurv$plot + labs(
  title    = "Kaplan-Meier Curves for overall survival"                
  # subtitle = "Overall Survival (% of patients)"
)

# Changing the font size, style and color

ggsurv <- ggpar(
  ggsurv,
  font.title    = c(16, "bold", "black"),         
  font.subtitle = c(10, "bold.italic", "black"), 
  font.caption  = c(14, "plain", "black"),        
  font.x        = c(14, "bold.italic", "black"),          
  font.y        = c(14, "bold.italic", "black"),      
  font.xtickslab = c(12, "plain", "black"),
  legend = "top"
)

ggsurv
```

<br>
<br>

```{r, echo=FALSE}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

colnames(IPD.Brahmer.a) <- c("time", "event", "Nivolumab")

cox.model <- coxph(Surv(time, event) ~ Nivolumab, data = IPD.Brahmer.a) 

# Schoenfeld plot
par(mfrow=c(1,1))
plot(cox.zph(cox.model), main = "Schoenfeld Individual Test p: 0.002")
abline(h=0, col=2)
```

<br>
<br>

```{r echo=FALSE, fig.cap="Cox Proportional Hazards Model"}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

colnames(IPD.Brahmer.a) <- c("time", "event", "arm")

# fit coxph model
cox.model <- coxph(Surv(time, event) ~ arm, data = IPD.Brahmer.a) 

# format results into data frame with global p-values
cox.model %>%
  tbl_regression(
    show_single_row = arm,
    label = arm ~ "Nivolumab vs Docetaxel",
    exponentiate = TRUE) %>%
  bold_p(t = 0.10) %>%
  bold_labels() %>%
  italicize_levels()
```

<br>
<br>

----------------------------------------------------------------------------------
  Average HR                           95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  0.846                              0.691, 1.0348                  0.104   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher) Weighted Cox Proportional Hazards Model

At a given instant in time a patient recieving the Nivolumab treatment is 0.846 times as likely to die as a patient who is on the Docetaxel treatment. The 95% confidence interval was (0.691, 1.0348) with p value 0.104.

<br>
<br>

----------------------------------------------------------------------------------
  $\hat{c}^{\prime} \%$                95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  45.82%                             40.87, 50.85                   0.104   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher1) Weighted Cox Proportional Hazards Model (Generalized Concordance)

With a probability of 45.82% (95% CI:45.8%, 52.5%), a patient treated with Nivolumab expected (at the baseline value) to die earlier than a patient treated with Docetaxel with p value 0.104.

<br>
<br>

```{r, echo=FALSE, fig.cap="Weights used by weighted Cox regression are plotted against time"}
knitr::include_graphics(path = "figure/brehmer.coxphw.svg")
```
In this study the increase in the censoring weights are small during the first nine months of treatment, after this they begin to rise rapidly. However, it should be noted that the normalised total weight range is [1, 2]. 

<br>
<br>

```{r, echo=FALSE, fig.cap="Weighted Log Rank Test"}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

IPD.Brahmer.a$event <- ifelse(IPD.Brahmer.a$event == 0, 1, 0)

colnames(IPD.Brahmer.a) <- c("time", "event", "arm")

result <- wlr.Stat(surv=IPD.Brahmer.a$time, cnsr=IPD.Brahmer.a$event, trt= IPD.Brahmer.a$arm, fparam=list(rho=c(0,0,1,1), gamma=c(0,1,1,0), wlr='FH(0,1)', APPLE=NULL))

result %>% 
  kableExtra::kable()
```

<br>
<br>

```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

IPD.Brahmer.a$event <- ifelse(IPD.Brahmer.a$event == 0, 1, 0)

IPD.Brahmer.a$arm <- ifelse(IPD.Brahmer.a$arm == 0, "control", "experimental")

rgs <- list(c(0, 0), c(0, 1), c(1, 1), c(1, 0))

draws <- 10

result.mc <- nphsim::combo.wlr(survival = IPD.Brahmer.a$time, cnsr = IPD.Brahmer.a$event, trt = IPD.Brahmer.a$arm, fparam = list(rgs=rgs,draws=draws))

result.mc <- lapply(result.mc, function(x) round(x, digits = 3))
```

```{r, echo=FALSE}
as.data.frame(result.mc) %>% kableExtra::kable()
```

<br>
<br>

```{r, include=FALSE, message=FALSE}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

IPD.Brahmer.a$arm <- ifelse(IPD.Brahmer.a$arm == 0, "control", "experimental")

IPD.Brahmer.a$event <- ifelse(IPD.Brahmer.a$event == 0, 1, 0)

result.rmst <- rmst.Stat(survival = IPD.Brahmer.a$time, cnsr = IPD.Brahmer.a$event, trt = IPD.Brahmer.a$arm, stra = NULL, fparam = 25)

result.rmst_ <- lapply(result.rmst, function(x) round(x, digits = 5))
```

```{r, echo=FALSE}
as.data.frame(result.rmst_) %>% kableExtra::kable()
```

```{r, include=FALSE}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

result.rmst2 = rmst2(time = IPD.Brahmer.a$time, status = IPD.Brahmer.a$event, arm = IPD.Brahmer.a$arm, tau = 25)

result.rmst2
```

The point estimate indicated that patients on the Nivolumab treatment survive `r round(result.rmst2$unadjusted.result[1,1],1)` months longer than those on Docetaxel group on average, when following up the patients `r round(result.rmst2$tau, 0)` months. The 95% confidence interval was (`r round(result.rmst2$unadjusted.result[1,2],1)` to `r round(result.rmst2$unadjusted.result[1,3],1)`) with p value `r round(result.rmst2$unadjusted.result[1,4],2)`.

```{r, echo=FALSE}
source("Libraries.R")

IPD.Brahmer.a <- readRDS("data/IPD.Brahmer.a.RDS")

result.rmst2 = rmst2(time = IPD.Brahmer.a$time, status = IPD.Brahmer.a$event, arm = IPD.Brahmer.a$arm, tau = 25)

plot(
result.rmst2,
xlab = "Months",
ylab = "Probability",
col = "black",
col.RMST = "#E7B800", 
col.RMTL = "#2E9FDF",
density = 80,
angle = 85
)
```

<br>
<br>

\clearpage 

## Mok et al. 2009 IPASS Study: Dimishing Treatment Effect

```{r, echo = FALSE, fig.cap="Patients with unknown EGFR mutation status", warning=FALSE, message=FALSE}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

km_trt_fit <- survfit(Surv(time, event) ~ arm, data = IPD.Mok.C)

ggsurv <- ggsurvplot(
  km_trt_fit,                     # survfit object with calculated statistics.
  data = IPD.Mok.C,             # data used to fit survival curves.
  risk.table = TRUE,       # show risk table.
  pval = FALSE,             # show p-value of log-rank test.
  conf.int = FALSE,         # show confidence intervals for 
  # point estimates of survival curves.
  palette = c("#E7B800", "#2E9FDF"),
  xlim = c(0,12),         # present narrower X axis, but not affect
  # survival estimates.
  xlab = "Months since Randomization",   # customize X axis label.
  ylab = "Pr of Progression-free Survival",   # customize X axis label.
  break.time.by = 4,     # break X axis in time intervals by 500.
  ggtheme = theme_light(), # customize plot and risk table with a theme.
  risk.table.y.text.col = T,# colour risk table text annotations.
  risk.table.height = 0.25, # the height of the risk table
  risk.table.y.text = FALSE,# show bars instead of names in text annotations
  # in legend of risk table.
  ncensor.plot = FALSE,      # plot the number of censored subjects at time t
  ncensor.plot.height = 0.25,
  conf.int.style = "step",  # customize style of confidence intervals
  surv.median.line = "hv",  # add the median survival pointer.
  legend.labs = c("Carboplatin", "Gefitinib")    # change legend labels.
)

# Labels for Survival Curves (plot)
ggsurv$plot <- ggsurv$plot + labs(
  title    = "Kaplan-Meier Curves for Progression-free Survival",                  
  subtitle = "Patients with unknown EGFR mutation status"
)

# Changing the font size, style and color

ggsurv <- ggpar(
  ggsurv,
  font.title    = c(16, "bold", "black"),         
  font.subtitle = c(10, "bold.italic", "black"), 
  font.caption  = c(14, "plain", "black"),        
  font.x        = c(14, "bold.italic", "black"),          
  font.y        = c(14, "bold.italic", "black"),      
  font.xtickslab = c(12, "plain", "black"),
  legend = "top"
)

ggsurv
```

<br>
<br>

```{r, echo=FALSE}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

colnames(IPD.Mok.C) <- c("time", "event", "Gefitinib")

cox.model <- coxph(Surv(time, event) ~ Gefitinib, data = IPD.Mok.C) 

# Schoenfeld plot
par(mfrow=c(1,1))
plot(cox.zph(cox.model), main = "Schoenfeld Individual Test p: 0.00")
abline(h=0, col=2)
```

<br>
<br>

```{r echo=FALSE, fig.cap="Cox Proportional Hazards Model"}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

colnames(IPD.Mok.C) <- c("time", "event", "arm")

# fit coxph model
cox.model <- coxph(Surv(time, event) ~ arm, data = IPD.Mok.C) 

# format results into data frame with global p-values
cox.model %>%
  tbl_regression(
    show_single_row = arm,
    label = arm ~ "Gefitinib vs Carboplatin-Paclitaxel",
    exponentiate = TRUE) %>%
  bold_p(t = 0.10) %>%
  bold_labels() %>%
  italicize_levels()
```

<br>
<br>

----------------------------------------------------------------------------------
  Average HR                           95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  3.863                              2.699   5.531                   0.000   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher) Weighted Cox Proportional Hazards Model

At a given instant in time a patient recieving the Nivolumab treatment is 3.863 times as likely to die as a patient who is on the Docetaxel treatment. The 95% confidence interval was (2.699, 5.531) with p value 0.00 .

<br>
<br>

----------------------------------------------------------------------------------
  $\hat{c}^{\prime} \%$                95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  79.44%                             72.96, 84.69                   0.000    
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher1) Weighted Cox Proportional Hazards Model (Generalized Concordance)

With a probability of 79.44% (95% CI:72.96%, 84.69%), a patient treated with Gefitinib expected (at the baseline value) to die earlier than a patient treated with Carboplatin-Paclitaxel with p value 0.000.

<br>
<br>

```{r, echo=FALSE, fig.cap="Weights used by weighted Cox regression are plotted against time"}
knitr::include_graphics(path = "figure/mok.c.coxph.svg")
```
In this study the increase in the censoring weights are large during the course of the treatment. However, it should be noted that the normalised total weight range is [0, 2]. 

<br>
<br>

```{r, echo=FALSE, fig.cap="Weighted Log Rank Test"}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

IPD.Mok.C$event <- ifelse(IPD.Mok.C$event == 0, 1, 0)

IPD.Mok.C$arm <- ifelse(IPD.Mok.C$arm == 1, "control", "experimental")

colnames(IPD.Mok.C) <- c("time", "event", "arm")

result <- wlr.Stat(surv=IPD.Mok.C$time, cnsr=IPD.Mok.C$event, trt= IPD.Mok.C$arm, fparam=list(rho=c(0,0,1,1), gamma=c(0,1,1,0), wlr='FH(0,1)', APPLE=NULL))

result %>% 
  kableExtra::kable()
```

<br>
<br>

```{r, echo=FALSE, include=FALSE}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

rgs <- list(c(0, 0), c(0, 1), c(1, 1), c(1, 0))

draws <- 10

IPD.Mok.C$event <- ifelse(IPD.Mok.C$event == 0, 1, 0)

IPD.Mok.C$arm <- ifelse(IPD.Mok.C$arm == 1, "control", "experimental")

result.mc <- nphsim::combo.wlr(survival = IPD.Mok.C$time, cnsr = IPD.Mok.C$event, trt = IPD.Mok.C$arm, fparam = list(rgs=rgs,draws=draws))

result.mc <- lapply(result.mc, function(x) round(x, digits = 3))
```

```{r, echo=FALSE}
as.data.frame(result.mc) %>% kableExtra::kable()
```

<br>
<br>
  
```{r, include=FALSE, message=FALSE}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

IPD.Mok.C$arm <- ifelse(IPD.Mok.C$arm == 0, "control", "experimental")

IPD.Mok.C$event <- ifelse(IPD.Mok.C$event == 0, 1, 0)

result.rmst <- rmst.Stat(survival = IPD.Mok.C$time, cnsr = IPD.Mok.C$event, trt = IPD.Mok.C$arm, stra = NULL, fparam = 12)

result.rmst_ <- lapply(result.rmst, function(x) round(x, digits = 5))
```

```{r, echo=FALSE}
as.data.frame(result.rmst_) %>% kableExtra::kable()
```

```{r, include=FALSE}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

result.rmst2 = rmst2(time = IPD.Mok.C$time, status = IPD.Mok.C$event, arm = IPD.Mok.C$arm, tau = 12)

result.rmst2
```

The point estimate indicated that patients on the Gefitinib treatment survive `r round(result.rmst2$unadjusted.result[1,1],1)` months longer than those on Carboplatin-Paclitaxel group on average, when following up the patients `r result.rmst2$tau` months. The 95% confidence interval was (`r round(result.rmst2$unadjusted.result[1,2],1)` to `r round(result.rmst2$unadjusted.result[1,3],1)`) with p value `r round(result.rmst2$unadjusted.result[1,4],1)`.

```{r, echo=FALSE}
source("Libraries.R")

IPD.Mok.C <- readRDS("data/IPD.Mok.C.RDS")

result.rmst2 = rmst2(time = IPD.Mok.C$time, status = IPD.Mok.C$event, arm = IPD.Mok.C$arm, tau = 12)

plot(
result.rmst2,
xlab = "Months",
ylab = "Probability",
col = "black",
col.RMST = "#E7B800", 
col.RMTL = "#2E9FDF",
density = 80,
angle = 85
)
```

<br>
<br>

\clearpage 

## Ohtsu et al. 2011 AVAGAST Study: Dimishing Treatment Effect

```{r, echo=FALSE, fig.cap="Patients in the intention-to-treat population", warning=FALSE, message=FALSE}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

km_trt_fit <- survfit(Surv(time, event) ~ arm, data = IPD.Ohtsu)

ggsurv <- ggsurvplot(
  km_trt_fit,               # survfit object with calculated statistics.
  data = IPD.Ohtsu,         # data used to fit survival curves.
  risk.table = TRUE,        # show risk table.
  pval = FALSE,             # show p-value of log-rank test.
  conf.int = FALSE,         # show confidence intervals for 
  # point estimates of survival curves.
  palette = c("#E7B800", "#2E9FDF"),
  xlim = c(0,21),         # present narrower X axis, but not affect
  # survival estimates.
  xlab = "Months since Randomization",   # customize X axis label.
  ylab = "Survival (probability)",   # customize X axis label.
  break.time.by = 3,     # break X axis in time intervals by 500.
  ggtheme = theme_light(), # customize plot and risk table with a theme.
  risk.table.y.text.col = T,# colour risk table text annotations.
  risk.table.height = 0.25, # the height of the risk table
  risk.table.y.text = FALSE,# show bars instead of names in text annotations
  # in legend of risk table.
  ncensor.plot = FALSE,      # plot the number of censored subjects at time t
  ncensor.plot.height = 0.25,
  conf.int.style = "step",  # customize style of confidence intervals
  surv.median.line = "hv",  # add the median survival pointer.
  legend.labs = c("Placebo", "Bevacizumab")    # change legend labels.
)

# Labels for Survival Curves (plot)
ggsurv$plot <- ggsurv$plot + labs(
  title    = "Kaplan-Meier estimates of overall survival",                  
  subtitle = "Patients in the intention-to-treat population"
)

# Changing the font size, style and color

ggsurv <- ggpar(
  ggsurv,
  font.title    = c(16, "bold", "black"),         
  font.subtitle = c(10, "bold.italic", "black"), 
  font.caption  = c(14, "plain", "black"),        
  font.x        = c(14, "bold.italic", "black"),          
  font.y        = c(14, "bold.italic", "black"),      
  font.xtickslab = c(12, "plain", "black"),
  legend = "top"
)

ggsurv
```

<br>
<br>

```{r, echo=FALSE}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

colnames(IPD.Ohtsu) <- c("time", "event", "Bevacizumab")

cox.model <- coxph(Surv(time, event) ~ Bevacizumab, data = IPD.Ohtsu) 

# Schoenfeld plot
par(mfrow=c(1,1))
plot(cox.zph(cox.model), main = "Schoenfeld Individual Test p: 0.0139")
abline(h=0, col=2)
```

<br>
<br>

```{r, echo=FALSE, fig.cap="Cox Proportional Hazards Model"}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

colnames(IPD.Ohtsu) <- c("time", "event", "arm")

# fit coxph model
cox.model <- coxph(Surv(time, event) ~ arm, data = IPD.Ohtsu) 

# format results into data frame with global p-values
cox.model %>%
  tbl_regression(
    show_single_row = arm,
    label = arm ~ "Bevacizumab vs Placebo",
    exponentiate = TRUE) %>%
  bold_p(t = 0.10) %>%
  bold_labels() %>%
  italicize_levels()
```

<br>
<br>

----------------------------------------------------------------------------------
  Average HR                           95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  0.852                              0.716, 1.015                   0.073  
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher) Weighted Cox Proportional Hazards Model

At a given instant in time a patient recieving the Nivolumab treatment is 0.852 times as likely to die as a patient who is on the Docetaxel treatment. The 95% confidence interval was (0.716, 1.015) with p value 0.073.

<br>
<br>

----------------------------------------------------------------------------------
  $\hat{c}^{\prime} \%$                95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  46.01%                            41.71, 50.37                   0.073   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher1) Weighted Cox Proportional Hazards Model (Generalized Concordance)

With a probability of 46.01% (95% CI:41.71%, 50.37%), a patient treated with Gefitinib expected (at the baseline value) to die earlier than a patient treated with Carboplatin-Paclitaxel with p value 0.073.

<br>
<br>

```{r, echo=FALSE, fig.cap="Weights used by weighted Cox regression are plotted against time"}
knitr::include_graphics(path = "figure/ohtsu.coxphw.svg")
```
In this study the increase in the censoring weights are small during the first 12 months of treatment, after this they begin to rise rapidly. However, it should be noted that the normalised total weight range is [1,25, 2]. 

<br>
<br>

```{r, echo=FALSE, fig.cap="Weighted Log Rank Test"}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

IPD.Ohtsu$event <- ifelse(IPD.Ohtsu$event == 0, 1, 0)

colnames(IPD.Ohtsu) <- c("time", "event", "arm")

result <- wlr.Stat(surv=IPD.Ohtsu$time, cnsr=IPD.Ohtsu$event, trt= IPD.Ohtsu$arm, fparam=list(rho=c(0,0,1,1), gamma=c(0,1,1,0), wlr='FH(0,1)', APPLE=NULL))

result %>% 
  kableExtra::kable()
```

<br>
<br>

```{r, echo=FALSE, include=FALSE}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

IPD.Ohtsu$event <- ifelse(IPD.Ohtsu$event == 0, 1, 0)

IPD.Ohtsu$arm <- ifelse(IPD.Ohtsu$arm == 0, "control", "experimental")

rgs <- list(c(0, 0), c(0, 1), c(1, 0), c(1, 1))

draws <- 10

result.mc <- nphsim::combo.wlr(survival = IPD.Ohtsu$time, cnsr = IPD.Ohtsu$event, trt = IPD.Ohtsu$arm, fparam = list(rgs=rgs,draws=draws))

result.mc <- lapply(result.mc, function(x) round(x, digits = 5))
```

```{r, echo=FALSE}
as.data.frame(result.mc) %>% kableExtra::kable()
```

<br>
<br>
  
```{r, include=FALSE, message=FALSE}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

IPD.Ohtsu$arm <- ifelse(IPD.Ohtsu$arm == 0, "control", "experimental")

IPD.Ohtsu$event <- ifelse(IPD.Ohtsu$event == 0, 1, 0)

result.rmst <- rmst.Stat(survival = IPD.Ohtsu$time, cnsr = IPD.Ohtsu$event, trt = IPD.Ohtsu$arm, stra = NULL, fparam = 22)

result.rmst_ <- lapply(result.rmst, function(x) round(x, digits = 5))
```

```{r, echo=FALSE}
as.data.frame(result.rmst_) %>% kableExtra::kable()
```

```{r, include=FALSE}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

result.rmst2 = rmst2(time = IPD.Ohtsu$time, status = IPD.Ohtsu$event, arm = IPD.Ohtsu$arm, tau = 22)

result.rmst2
```

The point estimate indicated that patients on the Gefitinib treatment survive `r round(result.rmst2$unadjusted.result[1,1],1)` months longer than those on Carboplatin-Paclitaxel group on average, when following up the patients `r result.rmst2$tau` months. The 95% confidence interval was (`r round(result.rmst2$unadjusted.result[1,2],1)` to `r round(result.rmst2$unadjusted.result[1,3],1)`) with p value `r round(result.rmst2$unadjusted.result[1,4],1)`.

```{r, echo=FALSE}
source("Libraries.R")

IPD.Ohtsu <- readRDS("data/IPD.Ohtsu.RDS")

result.rmst2 = rmst2(time = IPD.Ohtsu$time, status = IPD.Ohtsu$event, arm = IPD.Ohtsu$arm, tau = 22)

plot(
result.rmst2,
xlab = "Months",
ylab = "Probability",
col = "black",
col.RMST = "#E7B800", 
col.RMTL = "#2E9FDF",
density = 80,
angle = 85
)
```

<br>
<br>

\clearpage 

## Ascierto et al. 2017 The Ipilimumab Study: Delayed Treatment Effect

```{r, echo=FALSE, fig.cap="Patients in the intention-to-treat population", warning=FALSE, message=FALSE}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

km_trt_fit <- survfit(Surv(time, event) ~ arm, data = IPD.ascierto.2.a)

ggsurv <- ggsurvplot(
  km_trt_fit,               # survfit object with calculated statistics.
  data = IPD.ascierto.2.a,  # data used to fit survival curves.
  risk.table = TRUE,        # show risk table.
  pval = FALSE,             # show p-value of log-rank test.
  conf.int = FALSE,         # show confidence intervals for 
  # point estimates of survival curves.
  palette = c("#E7B800", "#2E9FDF"),
  xlim = c(0,45),         # present narrower X axis, but not affect
  # survival estimates.
  xlab = "Time since randomisation (months)",   # customize X axis label.
  ylab = "Overall survival (%)",   # customize X axis label.
  break.time.by = 3,     # break X axis in time intervals by 500.
  ggtheme = theme_light(), # customize plot and risk table with a theme.
  risk.table.y.text.col = T,# colour risk table text annotations.
  risk.table.height = 0.25, # the height of the risk table
  risk.table.y.text = FALSE,# show bars instead of names in text annotations
  # in legend of risk table.
  ncensor.plot = FALSE,      # plot the number of censored subjects at time t
  ncensor.plot.height = 0.25,
  conf.int.style = "step",  # customize style of confidence intervals
  surv.median.line = "hv",  # add the median survival pointer.
  legend.labs = c("3mg", "10mg")    # change legend labels.
)

# Labels for Survival Curves (plot)
ggsurv$plot <- ggsurv$plot + labs(
  title    = "Kaplan-Meier Curves for Over-all Survival",                  
  subtitle = "Overall survival in the intention-to-treat population."
)

# Changing the font size, style and color

ggsurv <- ggpar(
  ggsurv,
  font.title    = c(16, "bold", "black"),         
  font.subtitle = c(10, "bold.italic", "black"), 
  font.caption  = c(14, "plain", "black"),        
  font.x        = c(14, "bold.italic", "black"),          
  font.y        = c(14, "bold.italic", "black"),      
  font.xtickslab = c(12, "plain", "black"),
  legend = "top"
)

ggsurv
```

<br>
<br>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

colnames(IPD.ascierto.2.a) <- c("time", "event", "10mg")

cox.model <- coxph(Surv(time, event) ~ IPD.ascierto.2.a$`10mg`, data = IPD.ascierto.2.a) 

# Schoenfeld plot
par(mfrow=c(1,1))
plot(cox.zph(cox.model), main = "Schoenfeld Individual Test p: 0.1812")
abline(h=0, col=2)
```

<br>
<br>

```{r echo=FALSE, fig.cap="Cox Proportional Hazards Model"}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

colnames(IPD.ascierto.2.a) <- c("time", "event", "arm")

# fit coxph model
cox.model <- coxph(Surv(time, event) ~ arm, data = IPD.ascierto.2.a) 

# format results into data frame with global p-values
cox.model %>%
  tbl_regression(
    show_single_row = arm,
    label = arm ~ "10mg vs 3mg",
    exponentiate = TRUE) %>%
  bold_p(t = 0.10) %>%
  bold_labels() %>%
  italicize_levels()
```

<br>
<br>

----------------------------------------------------------------------------------
  Average HR                           95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  0.884                              0.739, 1.058                   0.179   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher) Weighted Cox Proportional Hazards Model

At a given instant in time a patient recieving the Nivolumab treatment is 0.846 times as likely to die as a patient who is on the Docetaxel treatment. The 95% confidence interval was (0.691, 1.0348) with p value 0.179.

<br>
<br>

----------------------------------------------------------------------------------
  $\hat{c}^{\prime} \%$                95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  46.93%                             42.5, 51.41                   0.179   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher1) Weighted Cox Proportional Hazards Model (Generalized Concordance)

With a probability of 46.93% (95% CI:42.5%, 51.41%), a patient treated with Gefitinib expected (at the baseline value) to die earlier than a patient treated with Carboplatin-Paclitaxel with p value 0.179.

<br>
<br>

```{r, echo=FALSE, fig.cap="Weights used by weighted Cox regression are plotted against time"}
knitr::include_graphics(path = "figure/asc.coxphw.svg")
```
In this study the increase in the censoring weights are large during the first 3 months of treatment, after this they begin to rise slowly until after 40 months and then increase very quickly. However, it should be noted that the normalised total weight range is [0.5, 1.75]. 

<br>
<br>

```{r, echo=FALSE, fig.cap="Weighted Log Rank Test"}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

IPD.ascierto.2.a$event <- ifelse(IPD.ascierto.2.a$event == 0, 1, 0)

colnames(IPD.ascierto.2.a) <- c("time", "event", "arm")

result <- wlr.Stat(surv=IPD.ascierto.2.a$time, cnsr=IPD.ascierto.2.a$event, trt= IPD.ascierto.2.a$arm, fparam=list(rho=c(0,0,1,1), gamma=c(0,1,1,0), wlr='FH(0,1)', APPLE=NULL))

result %>% 
  kableExtra::kable()
```

<br>
<br>

```{r, echo=FALSE, include=FALSE}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

rgs <- list(c(0, 0), c(0, 1), c(1, 1), c(1, 0))

draws <- 10

IPD.ascierto.2.a$event <- ifelse(IPD.ascierto.2.a$event == 0, 1, 0)

IPD.ascierto.2.a$arm <- ifelse(IPD.ascierto.2.a$arm == 0, "control", "experimental")

result.mc <- nphsim::combo.wlr(survival = IPD.ascierto.2.a$time, cnsr = IPD.ascierto.2.a$event, trt = IPD.ascierto.2.a$arm, fparam = list(rgs=rgs,draws=draws))

result.mc <- lapply(result.mc, function(x) round(x, digits = 3))
```

```{r, echo=FALSE}
as.data.frame(result.mc) %>% kableExtra::kable()
```

<br>
<br>
  
```{r, include=FALSE, message=FALSE}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

IPD.ascierto.2.a$arm <- ifelse(IPD.ascierto.2.a$arm == 0, "control", "experimental")

IPD.ascierto.2.a$event <- ifelse(IPD.ascierto.2.a$event == 0, 1, 0)

result.rmst <- rmst.Stat(survival = IPD.ascierto.2.a$time, cnsr = IPD.ascierto.2.a$event, trt = IPD.ascierto.2.a$arm, stra = NULL, fparam = 45)

result.rmst_ <- lapply(result.rmst, function(x) round(x, digits = 5))
```

```{r, echo=FALSE}
as.data.frame(result.rmst_) %>% kableExtra::kable()
```

```{r, include=FALSE}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

result.rmst2 = rmst2(time = IPD.ascierto.2.a$time, status = IPD.ascierto.2.a$event, arm = IPD.ascierto.2.a$arm, tau = 45)

result.rmst2
```

The point estimate indicated that patients on the Ipilimumab 10 mg/kg treatment survive `r round(result.rmst2$unadjusted.result[1,1],1)` months longer than those on Ipilimumab 3 mg/kg group on average, when following up the patients `r result.rmst2$tau` months. The 95% confidence interval was (`r round(result.rmst2$unadjusted.result[1,2],1)` to `r round(result.rmst2$unadjusted.result[1,3],1)`) with p value `r round(result.rmst2$unadjusted.result[1,4],1)`.

```{r, echo=FALSE}
source("Libraries.R")

IPD.ascierto.2.a <- readRDS("data/IPD.ascierto.2.a.RDS")

result.rmst2 = rmst2(time = IPD.ascierto.2.a$time, status = IPD.ascierto.2.a$event, arm = IPD.ascierto.2.a$arm, tau = 45)

plot(
result.rmst2,
xlab = "Months",
ylab = "Probability",
col = "black",
col.RMST = "#E7B800", 
col.RMTL = "#2E9FDF",
density = 80,
angle = 85
)
```

<br>
<br>

\clearpage 

## Rittermeyer et al. 2017 The OAK Study: Delayed Treatment Effect

```{r, echo = FALSE, fig.cap="Patients in the WT population", warning=FALSE, message=FALSE}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

km_trt_fit <- survfit(Surv(time, event) ~ arm, data = IPD.Rittmeyer)

ggsurv <- ggsurvplot(
  km_trt_fit,               # survfit object with calculated statistics.
  data = IPD.Rittmeyer,     # data used to fit survival curves.
  risk.table = TRUE,        # show risk table.
  pval = FALSE,             # show p-value of log-rank test.
  conf.int = FALSE,         # show confidence intervals for 
  # point estimates of survival curves.
  palette = c("#E7B800", "#2E9FDF"),
  xlim = c(0,26),         # present narrower X axis, but not affect
  # survival estimates.
  xlab = "Months since Randomization",   # customize X axis label.
  ylab = "Pr of Progression-free Survival",   # customize X axis label.
  break.time.by = 2,     # break X axis in time intervals by 500.
  ggtheme = theme_light(), # customize plot and risk table with a theme.
  risk.table.y.text.col = T,# colour risk table text annotations.
  risk.table.height = 0.25, # the height of the risk table
  risk.table.y.text = FALSE,# show bars instead of names in text annotations
  # in legend of risk table.
  ncensor.plot = FALSE,      # plot the number of censored subjects at time t
  ncensor.plot.height = 0.25,
  conf.int.style = "step",  # customize style of confidence intervals
  surv.median.line = "hv",  # add the median survival pointer.
  legend.labs = c("Docetaxel","Atezolizumab")    # change legend labels.
)

# Labels for Survival Curves (plot)
ggsurv$plot <- ggsurv$plot + labs(
  title    = "Kaplan Meier Estimates of Progression-free Survival",                  
  subtitle = "Patients in the IIT population"
)

# Changing the font size, style and color

ggsurv <- ggpar(
  ggsurv,
  font.title    = c(16, "bold", "black"),         
  font.subtitle = c(10, "bold.italic", "black"), 
  font.caption  = c(14, "plain", "black"),        
  font.x        = c(14, "bold.italic", "black"),          
  font.y        = c(14, "bold.italic", "black"),      
  font.xtickslab = c(12, "plain", "black"),
  legend = "top"
)

ggsurv
```

<br>
<br>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

colnames(IPD.Rittmeyer) <- c("time", "event", "Atezolizumab")

cox.model <- coxph(Surv(time, event) ~ IPD.Rittmeyer$`Atezolizumab`, data = IPD.Rittmeyer) 

# Schoenfeld plot
par(mfrow=c(1,1))
plot(cox.zph(cox.model), main = "Schoenfeld Individual Test p: 0.3971")
abline(h=0, col=2)
```

<br>
<br>

```{r echo=FALSE, fig.cap="Cox Proportional Hazards Model"}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

colnames(IPD.Rittmeyer) <- c("time", "event", "arm")

# fit coxph model
cox.model <- coxph(Surv(time, event) ~ arm, data = IPD.Rittmeyer) 

# format results into data frame with global p-values
cox.model %>%
  tbl_regression(
    show_single_row = arm,
    label = arm ~ "Atezolizumab versus Docetaxel",
    exponentiate = TRUE) %>%
  bold_p(t = 0.10) %>%
  bold_labels() %>%
  italicize_levels()
```

<br>
<br>

----------------------------------------------------------------------------------
  Average HR                           95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  0.728                              0.614, 0.863                   0.000   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher) Weighted Cox Proportional Hazards Model

At a given instant in time a patient recieving the Atezolizumab treatment is 0.0.728 times as likely to die as a patient who is on the Docetaxel treatment. The 95% confidence interval was (0.614, 0.863) with p value 0.000.

<br>
<br>

----------------------------------------------------------------------------------
  $\hat{c}^{\prime} \%$                95% CI	                      p-value
------------------------- ---------------------------------- ---------------------
  42.13%                            38.05, 46.31                     0.000   
------------------------- ---------------------------------- ---------------------
Table: (\#tab:inher1) Weighted Cox Proportional Hazards Model (Generalized Concordance)

With a probability of 49.12% (95% CI:45.8%, 52.5%), a patient treated with Gefitinib expected (at the baseline value) to die earlier than a patient treated with Carboplatin-Paclitaxel with p value 0.000.

<br>
<br>

```{r, echo=FALSE, fig.cap="Weights used by weighted Cox regression are plotted against time"}
knitr::include_graphics(path = "figure/ritt.coxphw.svg")
```
In this study the increase in the censoring weights are small during the first 18 months of treatment, after this they begin to rise rapidly. However, it should be noted that the normalised total weight range is [1.5, 2]. 

<br>
<br>

```{r, echo=FALSE, fig.cap="Weighted Log Rank Test"}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

IPD.Rittmeyer$event <- ifelse(IPD.Rittmeyer$event == 0, 1, 0)

colnames(IPD.Rittmeyer) <- c("time", "event", "arm")

result <- wlr.Stat(surv=IPD.Rittmeyer$time, cnsr=IPD.Rittmeyer$event, trt= IPD.Rittmeyer$arm, fparam=list(rho=c(0,0,1,1), gamma=c(0,1,1,0), wlr='FH(0,1)', APPLE=NULL))

result %>% 
  kableExtra::kable()
```

<br>
<br>

```{r, echo=FALSE, include=FALSE}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

IPD.Rittmeyer$event <- ifelse(IPD.Rittmeyer$event == 0, 1, 0)

IPD.Rittmeyer$arm <- ifelse(IPD.Rittmeyer$arm == 0, "control", "experimental")

rgs <- list(c(0, 0), c(0, 1), c(1, 1), c(1, 0))

draws <- 10

result.mc <- nphsim::combo.wlr(survival = IPD.Rittmeyer$time, cnsr = IPD.Rittmeyer$event, trt = IPD.Rittmeyer$arm, fparam = list(rgs=rgs,draws=draws))

result.mc <- lapply(result.mc, function(x) round(x, digits = 3))
```

```{r, echo=FALSE}
as.data.frame(result.mc) %>% kableExtra::kable()
```

<br>
<br>
  
```{r, include=FALSE, message=FALSE}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

IPD.Rittmeyer$arm <- ifelse(IPD.Rittmeyer$arm == 0, "control", "experimental")

IPD.Rittmeyer$event <- ifelse(IPD.Rittmeyer$event == 0, 1, 0)

result.rmst <- rmst.Stat(survival = IPD.Rittmeyer$time, cnsr = IPD.Rittmeyer$event, trt = IPD.Rittmeyer$arm, stra = NULL, fparam = 26)

result.rmst_ <- lapply(result.rmst, function(x) round(x, digits = 5))
```

```{r, echo=FALSE}
as.data.frame(result.rmst_) %>% kableExtra::kable()
```

```{r, include=FALSE}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

result.rmst2 = rmst2(time = IPD.Rittmeyer$time, status = IPD.Rittmeyer$event, arm = IPD.Rittmeyer$arm, tau = 26)

result.rmst2
```

The point estimate indicated that patients on the Atezolizumab treatment survive `r round(result.rmst2$unadjusted.result[1,1],1)` months longer than those on Docetaxel group on average, when following up the patients `r result.rmst2$tau` months. The 95% confidence interval was (`r round(result.rmst2$unadjusted.result[1,2],1)` to `r round(result.rmst2$unadjusted.result[1,3],1)`) with p value `r round(result.rmst2$unadjusted.result[1,4],1)`.

```{r, echo=FALSE}
source("Libraries.R")

IPD.Rittmeyer <- readRDS("data/IPD.Rittmeyer.RDS")

result.rmst2 = rmst2(time = IPD.Rittmeyer$time, status = IPD.Rittmeyer$event, arm = IPD.Rittmeyer$arm, tau = 26)

plot(
result.rmst2,
xlab = "Months",
ylab = "Probability",
col = "black",
col.RMST = "#E7B800", 
col.RMTL = "#2E9FDF",
density = 80,
angle = 85
)
```

<br>
<br>

## Tables

In addition to the tables that can be automatically generated from a data frame in **R** that you saw in [R Markdown Basics] using the `kable()` function, you can also create tables using _pandoc_. (More information is available at <https://pandoc.org/README.html#tables>.)  This might be useful if you don't have values specifically stored in **R**, but you'd like to display them in table form.  Below is an example.  Pay careful attention to the alignment in the table and hyphens to create the rows and columns.

----------------------------------------------------------------------------------
  Factors                    Correlation between Parents & Child      Inherited
------------------------- ----------------------------------------- --------------
  Education                                -0.49                         Yes
  
  Socio-Economic Status                     0.28                        Slight   
  
  Income                                    0.08                          No
  
  Family Size                               0.18                        Slight
  
  Occupational Prestige                     0.21                        Slight
------------------------- ----------------------------------------- --------------
Table: (\#tab:inher) Correlation of Inheritance Factors for Parents and Child 

We can also create a link to the table by doing the following: Table \@ref(tab:inher).  If you go back to [Loading and exploring data] and look at the `kable` table, we can create a reference to this max delays table too: Table \@ref(tab:maxdelays). The addition of the `(\#tab:inher)` option to the end of the table caption allows us to then make a reference to Table `\@ref(tab:label)`. Note that this reference could appear anywhere throughout the document after the table has appeared.  

<!-- We will next explore ways to create this label-ref link using figures. -->

\clearpage

<!-- clearpage ends the page, and also dumps out all floats.
  Floats are things like tables and figures. -->


## Figures

If your thesis has a lot of figures, _R Markdown_ might behave better for you than that other word processor.  One perk is that it will automatically number the figures accordingly in each chapter.    You'll also be able to create a label for each figure, add a caption, and then reference the figure in a way similar to what we saw with tables earlier.  If you label your figures, you can move the figures around and _R Markdown_ will automatically adjust the numbering for you.  No need for you to remember!  So that you don't have to get too far into LaTeX to do this, a couple **R** functions have been created for you to assist.  You'll see their use below.

<!--
One thing that may be annoying is the way _R Markdown_ handles "floats" like tables and figures (it's really \LaTeX's fault). \LaTeX\ will try to find the best place to put your object based on the text around it and until you're really, truly done writing you should just leave it where it lies. There are some optional arguments specified in the options parameter of the `label` function.  If you need to shift your figure around, it might be good to look here on tweaking the options argument:  <https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions>

If you need a graphic or tabular material to be part of the text, you can just put it inline. If you need it to appear in the list of figures or tables, it should be placed in a code chunk.
-->


In the **R** chunk below, we will load in a picture stored as `reed.jpg` in our main directory.  We then give it the caption of "Reed logo", the label of "reedlogo", and specify that this is a figure.  Make note of the different **R** chunk options that are given in the R Markdown file (not shown in the knitted document).

```{r reedlogo, fig.cap="Reed logo"}
include_graphics(path = "figure/reed.jpg")
```

Here is a reference to the Reed logo: Figure \@ref(fig:reedlogo).  Note the use of the `fig:` code here.  By naming the **R** chunk that contains the figure, we can then reference that figure later as done in the first sentence here.  We can also specify the caption for the figure via the R chunk option `fig.cap`.

\clearpage 

<!-- starts a new page and stops trying to place floats such as tables and figures -->

Below we will investigate how to save the output of an **R** plot and label it in a way similar to that done above.  Recall the `flights` dataset from Chapter \@ref(rmd-basics).  (Note that we've shown a different way to reference a section or chapter here.)  We will next explore a bar graph with the mean flight departure delays by airline from Portland for 2014.

```{r delaysboxplot, warnings=FALSE, messages=FALSE, fig.cap="Mean Delays by Airline", fig.width=6}
mean_delay_by_carrier <- flights %>%
  group_by(carrier) %>%
  summarize(mean_dep_delay = mean(dep_delay))
ggplot(mean_delay_by_carrier, aes(x = carrier, y = mean_dep_delay)) +
  geom_bar(position = "identity", stat = "identity", fill = "red")
```

Here is a reference to this image: Figure \@ref(fig:delaysboxplot).

A table linking these carrier codes to airline names is available at <https://github.com/ismayc/pnwflights14/blob/master/data/airlines.csv>.

\clearpage

Next, we will explore the use of the `out.extra` chunk option, which can be used to shrink or expand an image loaded from a file by specifying `"scale= "`. Here we use the mathematical graph stored in the "subdivision.pdf" file.

```{r subd, results="asis", echo=FALSE, fig.cap="Subdiv. graph", out.extra="scale=0.75"}
include_graphics("figure/subdivision.pdf")
```

Here is a reference to this image: Figure \@ref(fig:subd).  Note that `echo=FALSE` is specified so that the **R** code is hidden in the document.

**More Figure Stuff**

Lastly, we will explore how to rotate and enlarge figures using the `out.extra` chunk option.  (Currently this only works in the PDF version of the book.)

```{r subd2, results="asis", echo=FALSE, out.extra="angle=180, scale=1.1", fig.cap="A Larger Figure, Flipped Upside Down"}
include_graphics("figure/subdivision.pdf")
```

As another example, here is a reference: Figure \@ref(fig:subd2).  

## Footnotes and Endnotes

You might want to footnote something. ^[footnote text] The footnote will be in a smaller font and placed appropriately. Endnotes work in much the same way. More information can be found about both on the CUS site or feel free to reach out to <data@reed.edu>.

## Bibliographies

Of course you will need to cite things, and you will probably accumulate an armful of sources. There are a variety of tools available for creating a bibliography database (stored with the .bib extension).  In addition to BibTeX suggested below, you may want to consider using the free and easy-to-use tool called Zotero.  The Reed librarians have created Zotero documentation at <https://libguides.reed.edu/citation/zotero>.  In addition, a tutorial is available from Middlebury College at <https://sites.middlebury.edu/zoteromiddlebury/>.

_R Markdown_ uses _pandoc_ (<https://pandoc.org/>) to build its bibliographies.  One nice caveat of this is that you won't have to do a second compile to load in references as standard LaTeX requires. To cite references in your thesis (after creating your bibliography database), place the reference name inside square brackets and precede it by the "at" symbol.  For example, here's a reference to a book about worrying:  [@Molina1994].  This `Molina1994` entry appears in a file called `thesis.bib` in the `bib` folder.  This bibliography database file was created by a program called BibTeX.  You can call this file something else if you like (look at the YAML header in the main .Rmd file) and, by default, is to placed in the `bib` folder.  

For more information about BibTeX and bibliographies, see our CUS site (<https://web.reed.edu/cis/help/latex/index.html>)^[@reedweb2007]. There are three pages on this topic:  _bibtex_ (which talks about using BibTeX, at <https://web.reed.edu/cis/help/latex/bibtex.html>), _bibtexstyles_ (about how to find and use the bibliography style that best suits your needs, at <https://web.reed.edu/cis/help/latex/bibtexstyles.html>) and _bibman_ (which covers how to make and maintain a bibliography by hand, without BibTeX, at <https://web.reed.edu/cis/help/latex/bibman.html>). The last page will not be useful unless you have only a few sources.

If you look at the YAML header at the top of the main .Rmd file you can see that we can specify the style of the bibliography by referencing the appropriate csl file.  You can download a variety of different style files at <https://www.zotero.org/styles>.  Make sure to download the file into the csl folder.  

<!-- Fill the rest of the page with the content below for the PDF version. -->

\vfill

**Tips for Bibliographies**

- Like with thesis formatting, the sooner you start compiling your bibliography for something as large as thesis, the better. Typing in source after source is mind-numbing enough; do you really want to do it for hours on end in late April? Think of it as procrastination.
- The cite key (a citation's label) needs to be unique from the other entries.
- When you have more than one author or editor, you need to separate each author's name by the word "and" e.g. `Author = {Noble, Sam and Youngberg, Jessica},`.
- Bibliographies made using BibTeX (whether manually or using a manager) accept LaTeX markup, so you can italicize and add symbols as necessary.
- To force capitalization in an article title or where all lowercase is generally used, bracket the capital letter in curly braces.
- You can add a Reed Thesis citation^[@noble2002] option. The best way to do this is to use the phdthesis type of citation, and use the optional "type" field to enter "Reed thesis" or "Undergraduate thesis." 

## Anything else?

If you'd like to see examples of other things in this template, please contact the Data @ Reed team (email <data@reed.edu>) with your suggestions. We love to see people using _R Markdown_ for their theses, and are happy to help.


<!--chapter:end:03-chap3.Rmd-->

# Conclusion {-}

If we don't want Conclusion to have a chapter number next to it, we can add the `{-}` attribute.

**More info**

And here's some other random info: the first paragraph after a chapter title or section head _shouldn't be_ indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there.


<!--chapter:end:04-conclusion.Rmd-->

`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 


# The First Appendix

<!--
If you feel it necessary to include an appendix, it goes here.
-->

This first appendix includes all of the R chunks of code that were hidden throughout the document (using the `include = FALSE` chunk tag) to help with readibility and/or setup.

**In the main Rmd file**

```{r ref.label='include_packages', results='hide', echo = TRUE}
```

**In Chapter \@ref(ref-labels):**

```{r ref.label='include_packages_2', results='hide', echo = TRUE}
```

# The Second Appendix, for Fun

<!--chapter:end:05-appendix.Rmd-->

<!--
The bib chunk below must go last in this document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->

\backmatter

<!-- 
If you'd like to change the name of the bibliography to something else,
delete "References" and replace it.
-->

# References {-}
<!--
This manually sets the header for this unnumbered chapter.
-->
\markboth{References}{References}
<!--
To remove the indentation of the first entry.
-->
\noindent

<!--
To create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}


<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->

---
nocite: | 
  @angel2000, @angel2001, @angel2002a
...

<!--chapter:end:99-references.Rmd-->

