---
title: 'Methods to Accommodate NPH in the Analysis of Clinical Trials'
author: 'Martin Adrian Hewing'
date: "`r format(Sys.time(), '%d %B, %Y')`"
institution: 'University of Bath'
division: 'Data Science and Statistics'
advisor: 'Jonathan Bartlett'
# If you have more two advisors, un-silence line 7
#altadvisor: 'Your Other Advisor'
department: 'Mathematics'
degree: 'Master of Science'
knit: bookdown::render_book
site: bookdown::bookdown_site

# This will automatically install the {remotes} package and {thesisdown}
# Change this to FALSE if you'd like to install them manually on your own.
params:
  'Install needed packages for {thesisdown}': True
  
# Remove the hashtag to specify which version of output you would like.
# Can only choose one at a time.
output:
#  thesisdown::thesis_pdf: default 
   thesisdown::thesis_gitbook: default         
#  thesisdown::thesis_word: default
#  thesisdown::thesis_epub: default

# If you are creating a PDF you'll need to write your preliminary content 
# (e.g., abstract, acknowledgements) below or use code similar to line 25-26 
# for the .RMD files. If you are NOT producing a PDF, delete or silence
# lines 25-39 in this YAML header.
abstract: '`r if(knitr:::is_latex_output()) paste(readLines(here::here("prelims", "00-abstract.Rmd")), collapse = "\n  ")`'
# If you'd rather include the preliminary content in files instead of inline
# like below, use a command like that for the abstract above.  Note that a tab 
# is needed on the line after the `|`.
acknowledgements: |
  I want to thank a few people.
dedication: |
  You can have a dedication here if you wish. 
preface: |
  This is an example of a thesis setup to use the reed thesis document class 
  (for LaTeX) and the R bookdown package, in general.
  
# Specify the location of the bibliography below
bibliography: bib/thesis.bib
# Download your specific csl file and refer to it in the line below.
csl: csl/apa.csl
lot: true
lof: true
---


<!--
Above is the YAML (YAML Ain't Markup Language) header that includes a lot of 
metadata used to produce the document.  Be careful with spacing in this header!

If you'd prefer to not include a Dedication, for example, simply delete the section entirely, or silence them (add # before each line). 

If you have other LaTeX packages you would like to include, delete the # before header-includes and list the packages after hyphens on new lines.

If you'd like to include a comment that won't be produced in your resulting file enclose it in a block like this.

If you receive a duplicate label error after knitting, make sure to delete the index.Rmd file and then knit again.
-->

```{r include_packages, include=FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis.
if (!require(remotes)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("remotes", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste('You need to run install.packages("remotes")",
            "first in the Console.')
    )
  }
}
if (!require(thesisdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    remotes::install_github("ismayc/thesisdown")
  } else {
    stop(
      paste(
        "You need to run",
        'remotes::install_github("ismayc/thesisdown")',
        "first in the Console."
      )
    )
  }
}
library(thesisdown)
# Set how wide the R output will go
options(width = 70)
```

<!--
The ackowledgements, preface, dedication, and abstract are added into the PDF
version automatically by inputing them in the YAML at the top of this file.
Alternatively, you can put that content in files like 00--prelim.Rmd and
00-abstract.Rmd like done below.
-->



```{r eval=!knitr::is_latex_output(), child=here::here("prelims", "00--prelim.Rmd")}

```

```{r eval=!knitr::is_latex_output(), child=here::here("prelims", "00-abstract.Rmd")}

```

<!-- The {.unnumbered} option here means that the introduction will be 
"Chapter 0." You can also use {-} for no numbers on chapters.
-->

# Technical Section {.unnumbered}

## The Cox Proportional Hazards model

This paper will now proceed to give a concise outline of censoring, the hazard function, the Cox model and some of the most important characteristics of this particular approach.
<br/><br/>
Censoring happens when some information about the survival times of individuals is available, but it is not known precisely. In general there are three separate answers to why censoring is likely to occur. Firstly, the individual does not get the event before the end of the study. Secondly, in the course of the study period, an individual is lost to follow-up, and thirdly, because an individual withdraws from the study, but for a reason other than due to getting the event of interest.
<br/><br/>
Additionally, there are three types of censoring that can happen. Firstly, data that is right-censored. In this case the true time survived is the same as or more than the recorded survival time. Secondly, data that is left-censored. In this case the true time survived is the same as or less than the recorded survival time. Thirdly, data that is interval-censored. In this case the true time survived is within a time interval which is known to the researcher. This project will focus on data that is right censored.
<br/><br/>
The hazard function(also known as the conditional failure rate) is given by: 

<br/><br/>

$$h(t)=\lim _{\Delta t \rightarrow 0} \frac{P(t \leq T<t+\Delta t | T \geq t)}{\Delta t}$$

<br/><br/>

An explanation of the various components of the hazard function are as follows: $\mathit{h(t)}$ is equal to the limit as $\mathit\Delta t$ (gives instantaneous potential) gets closer to zero of the probability that an individual gets the event in the time interval $[t, t+\Delta t]$ given the individual has already survived up to time $\textit{t}$ divided by a small change in $\textit{t}$, noting that, $\mathit{h(t)}$ is not a probability as $\textit{P}$ has been divided by $\mathit\Delta t$. 
<br/><br/>
Formally $\mathit{h(t)}$ can be defined as a function that "gives the instantaneous potential per unit time for the event to occur, given that the individual has survived up to time $\textit{t}$". Alternatively, $\mathit{h(t)}$ can be thought of as like a speedometer. This device gives drivers information on their current velocity, for example a car travelling at 60 miles per hour, has the potential to cover 60 miles in the next hour. However, because the vehicle might accelerate or decelerate in that time period, at that moment when 60 MPH was observed, the speed reading does not show how many miles the car will actually cover, it just gives drivers velocity or instantaneous potential at that moment (this is assuming that it is given the driver has already driven some distance).
<br/><br/>
In a similar fashion the hazard function calculates the potential to get the event being studied at that instant, given survival up to that point in time. Furthermore, it is important to note that for specific $\textit{t}$ time values the hazard function has two properties: firstly, $\mathit{h(t)}$ is not limited to an upper bound value. Secondly, $\mathit{h(t)}$ is never negative, it is more than or equal to zero. 
<br/><br/>
The Cox proportional hazards regression model  formula is given by:

<br/><br/>

$$h(t, \mathbf{X})=h_{0}(t) e^{\sum_{i=1}^{p} \beta_{i} X_{i}}$$

<br/><br/>

$$\mathbf{X}=\left(X_{1}, X_{2}, \dots, X_{p}\right)$$

<br/><br/>

The left hand side of the above expression is the time $\textit{t}$ hazard that an individual faces given a collection of explanatory variables $\mathbf{X}$ whereas, the right hand side of (2) is the product of the baseline hazard function given by $\mathit{{h_{0}(t)}}$ and an exponential expression which is raised to the $\mathit{\beta_{i} X_{i}}$ summed over the $\textit{P}$ predictor $\mathit{X}$ variables. 
<br/><br/>
Additionally in regard to the proportional hazards assumption it is important to note that in the formula given in (2) the baseline hazard is a function of $\textit{t}$ but not the $X^{\prime} s$.  Conversely, the exponential expression does not involve the $\textit{t's}$, but does involve the $X^{\prime} s$. Therefore, the $X^{\prime} s$ are "time-independent" and can be defined as "any variable whose value for a given individual does not change over time". Moreover, two important properties of the Cox proportional hazards model regression model formula are that firstly when all the $X^{\prime} s$ are zero then the formula equals the baseline hazard function as an exponential raised to the zero equals one, and the exponential in the Cox formula ensures that the fitted hazard is not negative. 
<br/><br/>
Secondly, the Cox model is semi-parametric because $\mathit{{h_{0}(t)}}$ the baseline hazard function is unspecified, whereas , parametric models such as the Weibull or Exponential have a fixed functional form, so the Cox model is appropriate in many data situations where functional form of the baseline hazard is unknown as it can often accurately approximate the correct parametric form. 
<br/><br/>
The estimates of the $\beta^{\prime} \mathrm{s}$ (parameters) in the Cox model:  $h(t, \mathbf{X})=h_{0}(t) e^{\sum_{i=1}^{p} \beta_{i} X_{i}}$ are computed using a maximum likelihood approach to give the ML estimates $\hat{\beta}_{i}$. 
<br/><br/>
Another important concept is the hazard ratio. This ratio allows comparison of two individuals that are differentiated by their predictor set ($X^{\prime} s$). Computing the hazard ratio as follows:

<br/><br/>

$$\widehat{H R}=\frac{\hat{h}\left(t, \mathbf{X}^{*}\right)}{\hat{h}(t, \mathbf{X})}$$

<br/><br/>

$$\mathbf{X}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)$$

<br/><br/>

The hazard ratio is the division of $h\left(t, \mathbf{X}^{*}\right)$ by $h(t, \mathbf{X})$. Noting that the individual with the greater hazard is in the numerator (usually the placebo group) so that the hazard ratio is a value greater than one and $\hat{h}\left(t, \mathbf{X}^{*}\right) \geq \hat{h}(t, \mathbf{X})$, which aids in interpretability.

<br/><br/>

$$\widehat{H R}=\frac{\hat{h}_{0}(t) e^{\sum_{e=1}^{p} \hat{\beta}_{i} X_{i}^{*}}}{\hat{h}_{0}(t) e^{\sum_{e^{i=1}}^{p} \hat{\beta}_{i} X_{i}}}=e^{\sum_{i=1}^{p} \hat{\beta}_{i}\left(X_{i}^{*}-X_{i}\right)}$$

<br/><br/>

To arrive at the final formula in (4): Substitute the Cox model formula into the hazard ratio for the respective $\mathbf{X}^{*}$  and $\mathbf{X}$ values to express it in terms of the regression coefficients. Noting that the expression does not depend on $\textit{t}$.

<br/><br/>

The proportional hazards assumption meaning can now be explained, using the hazard ratio.

<br/><br/>

$$\hat{h}\left(t, X^{*}\right)=\hat{\theta} \hat{h}(t, X)$$

<br/><br/>

By letting $\hat{\theta}$ equal to $\exp \left[\sum_{i=1}^{p} \hat{\beta}_{i}\left(X_{i}^{*}-X_{i}\right)\right]$ it follows that $\frac{\hat{h}\left(t, \mathbf{X}^{*}\right)}{\hat{h}(t, \mathbf{X})}=\hat{\theta}$ and so $\hat{h}\left(t, X^{*}\right)=\hat{\theta} \hat{h}(t, X)$ where the constant of proportionality $\hat{\theta}$ is not dependent on $\textit{t}$. Therefore, one subjects hazard function is in proportion to another subjects hazard. 
Klein05

## Model Diagnostics

Moore D explains that Schoenfeld residuals can be used to assess the proportional hazards assumption:

The partial log-likelihood function is given by: 

<br>
<br>

$$l(\beta)=\sum_{i \in D}\left\{\log \left(\psi_{i}\right)-\log \left(\sum_{k \in R_{i}} \psi_{k}\right)\right\}=\sum_{i \in D}\left\{z_{i} \beta-\log \left(\sum_{k \in R_{i}} e^{z_{k} \beta}\right)\right\}$$

<br>
<br>

The score function is obtained by taking the derivative of the partial log-likelihood function:

<br>
<br>

$$l^{\prime}(\beta)=\sum_{i \in D}\left\{z_{i}-\sum_{k \in R_{i}} z_{k} \cdot p\left(\beta, z_{k}\right)\right\},$$
where
$$p\left(\beta, z_{k}\right)=\frac{e^{z_{k} \beta}}{\sum_{j \in R_{k}} e^{z_{j} \beta}}$$
The Schoenfeld residuals are the score function's individual terms, and every term is calculated by subtracting the expected values* from the observed values of the covariate for the $i^{th}$ patient.

*The expected value is $E\left(Z_{i}\right)=\bar{z}\left(t_{i}\right)$, this is a weighted sum (weight is $p_{k}(\beta)$) of the values of the covariates for at risk patients at that particular time point.

An alternative way to understand the weights is that they are the chance of choosing an indivual from the risk set at time $t$.

Therefore, the residual for $\hat{\beta}$ for the $i^{th}$ event time is:

<br>
<br>

$$\hat{r}_{i}=z_{i}-\sum_{k \in R_{i}} z_{k} \cdot p\left(\hat{\beta}, z_{k}\right)=z_{i}-\bar{z}\left(t_{i}\right)$$
<br>
<br>

When the proportional hazards assumption is not violated a plot of $\hat{r}_{i}$ against the covariate $z_{i}$ will create a collection of points that are centered at zero. Noting that the residuals apply to just the event and not the censoring times. 

<br>
<br>

A further development was put forward by Gramsch et al. in order to make the residuals easier to interpret. The researchers put forward the idea of scaling $\hat{r}_{i}$ by its variance estimate to create a scaled residual: 

<br>
<br>

$$r_{i}^{*}=r_{i} \cdot d \cdot \operatorname{var}(\hat{\beta})$$
where the sum of events is given by $d$ and $\operatorname{var}(\hat{\beta})$ is the parameter estimate variance. 

<br>
<br>

When the hazard ratio is a function of $\beta(t)$ and $t$ it follows that the expectation of the scaled residuals is: 

<br>
<br>

$$E\left(r_{i}^{*}\right) \approx \beta+\beta(t)$$

<br>
<br>

In this way an approximation of $\beta(t)$ can be obtained by adding the Cox Proportional Hazards estimate $\hat{\beta}$ to the standardised residuals.

<br>
<br>

Finally a Schoenfeld plot of $r_{i}^{*}$ versus $t$ or $log(t)$ can be created to examine the residuals.

## The Weighted Log Rank Test (WLRT)

Roychoudhury et al. explain that the most frequently used rank-based test to compare two survival curves is the log-rank test (LRT).

As the LRT is a non-parametric test, it performs well when the non-proportional hazards (NPH) assumption is not violated. However, when this assumption is broken the LRT is not sufficiently powerful. Furthermore, if the proportional hazards assumption is broken, it makes the hazard ratio (can be thought of as an average hazard ratio, which is subject to to the overall follow-up of study participants) difficult to interpret. 

Therefore, when NPH are present it is not apparent which estimand the hazard ratio and LRT are associated with. However, it should be noted that the LRT gives good performance in regard to type I error. This gives the rationale to consider the WLRT when NPH are present.

<br>
<br>

Jimenez et al. gives a definition of the WLRT: 

<br>
<br>

$$Z_{r}=\frac{\sum_{i=1}^{D} r_{i}\left(d_{1 i}-\mathrm{E}\left(d_{1 i}\right)\right)}{\sqrt{\sum_{i=1}^{D} r_{i}^{2} \operatorname{Var}\left(d_{1 i}\right)}}$$

$\text { where } \mathrm{E}\left(d_{1 i}\right)=n_{1 i} \times\left(\frac{d_{i}}{n_{i}}\right), \operatorname{Var}\left(d_{1 i}\right)=\frac{n_{1 i} n_{2 i} d_{i}\left(n_{i}-d_{i}\right)}{n_{i}^{2}\left(n_{i}-1\right)}$

$\begin{aligned} &\text { and } Z_{r}^{\text {appr. }} N(0,1) \text { under the null hypothesis }\ H 0: h_{1} / h_{2}=1\end{aligned}$

<br>
<br>

Fleming et al. developed the technique of using $r_{i}$ to weight early, middle or late differences via the $G^{\rho, \gamma}$ collection of weighted log rank tests. The time $t_{i}$ weight function is given by: 

<br>
<br>

$$r_{i}=\hat{S}\left(t_{i}\right)^{\rho}\left(1-\hat{S}\left(t_{i}\right)\right)^{\gamma}$$

Where $\hat{S}\left(t_{i}\right)$ is the Kaplan-Meier estimator.

<br>
<br>

In the above equation choices of $\rho$ and $\gamma$ values serve to create different weights for the WLRT. 

<br>

---------------------------------------------------------------------------- -------------------
  Rho $\rho$        Gamma $\gamma$     Difference            Function                Name              
--------------- -------------------- --------------- ----------------------- --------------------
      0                  0               None                 $W(t)$               Log Rank

      1                  0               Early                $S(t-)$         Prentice-Wilcoxon

      1                  1               Middle           $S(t-)*(1-S(t))$            -

      0                  1               Late                $1-S(t-)$                -
------------------------------------------------------------------------------------------------
Table: (\#tab:wlr) Weighted Log Rank Test Weight Functions 

<br>

The information in table \@ref(tab:wlr) is used to create four visualisations corresponding to the different $G^{\rho, \gamma}$ functions using the data from the [Mok et al. 2009 IPASS Study](#mok.a). 

```{r, include=FALSE}
source("Libraries.R")

IPD.Mok.A <- readRDS("data/IPD.Mok.A.RDS")

# Pooled Survival Function S(t-) or 1-S(t-) = S(t)
S <- survfit(Surv(time, event) ~ 1, data = IPD.Mok.A)
```

```{r, echo=FALSE, fig.cap="FH(0,0) Log-Rank Test"}
# Log.Rank 
plot(x = S$time, 
     y = rep(1, times = length(S$time)),
     main = "Constant Function One Vs Time", 
     xlab = "Time",
     ylab = "Constant Function 1",
     col = "blue")
```

If the NPH assumption were not violated this project would expect to see a hazard ratio of one throughout the treatment time period $t$. This suggests events will be evenly spaced accross $t$.

<br>
<br>

```{r, echo=FALSE}
# FH(1,0) S(t-) vs t (Prentice-Wilcoxon) (early effect)
plot(x = S$time, y = S$surv,
     main = "FH(1,0) S(t-) vs Time (Prentice-Wilcoxon) (Early effect)", 
     xlab = "Time",
     ylab = "S(t-)",
     col = "blue")
```

<br>
<br>

Lin et al. explain this weight would correspond to a early positive treatment effect. If the experimental treatment were to have the strongest positive impact on survival at the start of the treatment time period $t$ but this decreases as the treatment period progesses. This would suggest that more events will occur later in the study treatment period $t$.

<br>
<br>

```{r, echo=FALSE}
# FH(1,1) S(t)*(1-S(t)) (middle effect)
plot(x = S$time, 
     y = (S$surv)*(1-(S$surv)), 
     main = "FH(1,1) S(t)*(1-S(t)) (Middle effect)", 
     xlab = "Time",
     ylab = "S(t)*(1-S(t)",
     col = "blue")
```

<br>
<br>

Lin et al. explain this weight would correspond to a diminishing treatment effect, if the experimental treatment were to have the strongest impact on survival at the middle of the treatment time period $t$ and as the study progresses this effect tapers off and the experimental treatment becomes as effective as the standard of care. This suggests events will occur more often at the start and the end of the treatment period $t$.

<br>
<br>

```{r, echo=FALSE}
# FH(0,1) 1-S(t-) against t (late effect)
plot(x = S$time, y = (1-S$surv),
     main = "FH(0,1) S(t) vs Time (Late Effect)", 
     xlab = "Time",
     ylab = "1-S(t-)",
     col = "blue")
```

Conversley Lin et al. explain This weight would correspond to a delayed treatment effect, if the experimental treatment were to have the strongest impact on survival at the end of the treatment time period $t$ and as the study progresses this effect increases. This suggests events will occur less often as later time periods $t$ are reached.
