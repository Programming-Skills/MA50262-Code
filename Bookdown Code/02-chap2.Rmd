# Reconstructing Individual Patient Data  {#math-sci}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

There has been an increasing need for researchers within academia and/or the pharmaceutical industry to gain access to individual clinical trial patient data (IPD) so that they can perform various secondary analyses. However, due to the often-large sums that drug companies invest into research and development, these firms are understandably reluctant to grant free access to their data until they have fully made use of this resource. Therefore, there is the need to try to recreate IPD from published clinical trial reports through various methods. 

As a postgraduate student on an industrial placement this project had originally sought to make use of study data applied for under the placement companyâ€™s data sharing scheme in order to investigate different methods for handling non-proportional hazards. However, this request was rejected on the grounds that the data was still business sensitive. 

Through various industrial and academic sources, researchers are often able to access published Kaplan Meier (KM) curves and their corresponding summary statistics (CI and p-values), the numbers at risk in the study and potentially the total number of events witnessed in the clinical trial. 

From the published KM curves and the patient at risk data Wan et al. 2015 identified four methods that are capable of re-creating Individual Patient Data (IPD). 

## The Least Squares approach

This method can estimate the parameters via the minimisation of the sum of the squared residuals, where the residuals are the difference between the actual and estimated data. 

For example if 30 points of data $\left(\mathrm{x}_{1}, \mathrm{y}_{1}\right),\left(\mathrm{x}_{2}, \mathrm{y}_{2}\right), \ldots,\left(\mathrm{x}_{30}, \mathrm{y}_{30}\right)$ have been extracted from a KM curve, and are following a distribution that has a function in the form $\mathrm{y}=\mathrm{f}(\mathrm{x}, \beta)$ where $\beta=(\beta 1, \beta 2)$. The parameter $\beta$ vectors could be estimated via the method of Least Squares, in which the sum of the squared residuals has been minimised.

## Graphical Approach

A parametric model can be estimated via the graphical method. This approach requires the researcher to make a transformation of the survival function to a function with a linear form. A linear line is then fitted to the collection of points that have been extracted from the published KM curve. 

For example, the survival function of the Weibull distribution is: $$S(t)=e^{-\lambda t^{\gamma}}$$ Where $\gamma$ is the shape and $\lambda$ is the scale parameter respectively. 

A transformation of this distribution can be made by taking the logarithm of the preceding equation: $$\log (S(t))=-\lambda t_{\gamma}$$ 

Taking the logarithm for the second time gives: $$\log (-\log (S(t)))=\log (\lambda)+ \gamma *\log (t)$$
From this formula it is possible to plot $$\log (-\log (S(t)))\ versus\ \log (t)$$ and henceforth make an estimate of the parameters. 

## Interval-censored data estimation approach

Hoyle et al. put forward an approach to estimate IPD using the total number of patients, the respective numbers at risk, and the corresponding survival probabilities. However, this method assumes that there is constant censoring in each of the time intervals. Furthermore, the number of censorships and events in every time interval of a quarter length were estimated, which lead to a large improvement of the fit of the curves. Moreover, when information on patients at risk is not known, an estimate of the number of censorship and events can be made using the approach put forward by Tierney et al. 2007. Additionally, the authors note that the method will become more accurate if more intervals are used and that the estimated IPD will be interval censored. 

The researchers compared their approach to the two traditional methods mentioned above (Least Squares and the Graphical approach) using a Monte Carlo simulation. The elements included in the simulation were the various combinations of parameters in the Weibull distribution, the sample size and the type of censoring. The authors proved that their approach was superior to the traditional method with respect to both fitting the curve and the estimation of the mean survival time. Importantly, the estimate of the mean survival time using the Hoyle et al. approach was as accurate as an estimate that was obtained by calculating the maximum likelihood estimator (MLE) on the actual IPD. Additionally, it is worth noting that another benefit over the traditional methods is that this method can capture uncertainty. Several drawbacks, of the authors research are that they did not vary their censoring proportions and only the Weibull distribution was considered in their Monte Carlo simulation.

## Precise survival data approach

An approach developed by Guyot et al. 2012 that can be used with data that is left or right censored, makes use of an iterative algorithm. From this recreated IPD the survival curves can be reconstructed. In constrast to the approach put forward by Hoyle et al. the estimated IPD is precise survival data and not interval survival data. However, an assumption of Guyot et al. is that the rate of censoring is constant within a given time interval. 

A concise explaination of the Guyot et al. algorithm is now given: 

```{r, echo=FALSE, fig.cap="Guyot Algorithm Flowchart"}
knitr::include_graphics(path = "figure/guyot.flowchart.PNG")
```

The intitial estmate of the number of censored subjects is obtained for the $i^{th}$ interval. From this ititial estimate, a calculation of the number of censored subjects that occured between the extracted KM co-ordinate $k$ and $k+1$ is made. Now this calculation is used to estimate the number of events that occur at every extracted KM co-ordinate $k$ and also the numbers of patients at risk at $k+1$. This information is used to compare the estimate of the number of patients at risk at the start of the interval $i$ to the reported number at risk at the beginning of the interval $i$, and if the numbers are not equal the process repeats itself until the two numbers are equal and this matches the previous iteration. Conversely, in the situation where, the total number of the reported events is greater than the estimate of the number of events the process repeats itself until the two numbers equal from the previous iteration.

Guyot et al. evaluated the accuracy of their approach by comparing the findings to the reported statistics in four papers that they selected to recreate, choosing not to carry any simulations. These included 22 probabilites of survival, 7 times of median survival, 6 hazard ratios and 4 log hazard ratio standard errors. The researchers discovered that their approach was highly accurate for both median survival times and survival probabilities and the accuracy was acceptable when the total number of events or the number of patients at risk was given. 

## Evaluation of the Four Approaches

Wan et al. 2015 compared all four approaches on simulated data and found that the least squares and graphical methods performed badly due to their tendency to over-estimate by a large margin in certain situations. Furthermore, when the Weibull distribution was used both the Hoyle et al. and Guyot et al. methods gave good estimates and uncertainty values of the mean survival time. In-fact the accuracy of the estimated values was as accurate as those obtained using the actual IPD. However, when the lognormal distribution was used ($\sigma=2, \mu=0.303$ the hazard function falls over almost all values of time) the Guyot et al. approach was markedly more accurate (less MLE bias) when the rate of censoring was high 42% and 76% respectively, when compared to the Hoyle et al. approach (Shen et al. also found that the MLE approach was biased in the presence of heavily censored data). Moreover, Wu et al. noted that in many studies there is a high censoring rate, they estimate this to be as high as 70% in many clinical trials. Additionally, Tai et al. state that the lognormal distribution is much more widely used in fitting cancer site data. Therefore, this project concludes that in a situation where, there is a high level of censoring and the lognormal distribution is much more likely to be used it seems prudent to make use of the Guyot et al. approach over alternative methods.

## Implementation of the Guyot et al. Approach

To implement the Guyot et al. algorithm and reconstruct the IPD for a published KM curve a number of steps must be undertaken. Noting that it is imperative that the number of patients at risk and/or the total number of events must be known to apply this method.

Several reseachers have made use of the Guyot et al. algorithm in their work, this project will be using the method used by Satagopan et al. and some adjustments proposed by Magirr D and Biao et al. .

The first step was to source the studies that exhibit non-proportional hazards. (This project decided to source two studies for each type of non-proportional hazards, including, crossing hazards, a delayed treatment effect or diminishing treatment effect). 

```{r, echo=FALSE, fig.cap="Knezevic et al. Various Types of Non-Proportional Hazards"}
knitr::include_graphics(path = "figure/np.PNG")
```

The main source of research papers that feature non-proportional hazards have been via the techniques used in the literature review. This has involved a continued search via the Mathematical Sciences hub on the University of Bath library portal, various databases including "Pubmed" using a modified keyword search to locate studies that exhibit non-proportional hazards for example "Oncology non-proportional hazards" returned 878 results, while "Clinical Trial non-proportional hazards" returned 943 results. Furthermore, as this is a contemporary issue which is having a significant impact on clinical trials, especially those focusing on oncology and immunotherapy many pharmacuetical companies are now working together to discuss these challenges and propose solutions. The information from these working groups are often then freely distributed. An example of one such working group is: The public workshop hosted at the Duke, Robert J. Margolis, MD Center for Health Policy titled "Oncology Clinical Trials in the Presence of Non-Proportional Hazards". This session examined various studies that featured non-proportion hazards and how different approaches might be used to overcome the problem of the violation of non-proportional hazards, and a follow up research paper created by Lin et al. 2020 titled: "Alternative Analysis Methods for Time to Event Endpoints Under Nonproportional Hazards: A Comparative Analysis" (discussed in the literature review). 

Once the relevant studies featuring non-proportional hazards have been sourced, as per the method undertaken by Satagopan et al. the first stage is to load the png image into Adobe Illustrator and to convert the image into a high fidelity photo. This enables the separate KM curves to be isolated from each other and saved to separate jpeg files. Then the separate images can be loaded into a software program such as "DigitalizeIT" that will facilitate the digital extraction of the survival and time probabilities. In this software program the researcher must stipulate the minimum and maximum X and Y values respectively and trace the outline of the KM curve either automatically or via a manual approach which involves tracing the curve with multiple mouse clicks. This data is then introduced to the algorithm after the researcher has stipulated which time ranges to consider. Noting that each treatment arm must be considered separately The output result will be IPD for each treatment arm which the researcher can bind together to create IPD for each study. This was carried out for the six studies that were selected for this project. 

It is worth noting that an issue faced in this process was that the researcher must ensure that their data is monotonically decreasing and well defined. Furthermore, Biao et al. explains that if the researcher stipulates the number at risk parameter after the automatic collection of data points (the software can set too many instances of the same survival (y-axis) to different time points (x-axis)) there is the risk of upsetting the delicate balance between the amount of censored that are needed to ensure proportionality and overall numbers (of censored) between time points. If this relationship is broken then the vector of censored will contain negative numbers and break the algorithm. This problem occured during the creating of IPD for this project. However, a solution was to create a custom function that would remove values that where non-monotonically decreasing and to use the manual tracing method for the collection of some data-sets. 

```{r, echo=FALSE, fig.cap="Red points show Non-Monotonically Decreasing Data"}
knitr::include_graphics(path = "figure/mono3.PNG")
```

<br>
<br>

The Reconstructed vursus the Published Kaplan Meier Curves placed side by side for comparson.
```{r, echo=FALSE, fig.cap="Mok et al. 2009 Figure A - Reconstructed vs Published Kaplan Meier Curves and Patients at Risk"}
knitr::include_graphics(path = "figure/mok.combined.svg")
```

## Math

\TeX\ is the best way to typeset mathematics. Donald Knuth designed \TeX\ when he got frustrated at how long it was taking the typesetters to finish his book, which contained a lot of mathematics.  One nice feature of _R Markdown_ is its ability to read LaTeX code directly.

If you are doing a thesis that will involve lots of math, you will want to read the following section which has been commented out. If you're not going to use math, skip over or delete this next commented section.


<!-- MATH and PHYSICS majors: Uncomment the following section -->
<!--
$$\sum_{j=1}^n (\delta\theta_j)^2 \leq {{\beta_i^2}\over{\delta_i^2 + \rho_i^2}}
\left[ 2\rho_i^2 + {\delta_i^2\beta_i^2\over{\delta_i^2 + \rho_i^2}} \right] \equiv \omega_i^2
$$

From Informational Dynamics, we have the following (Dave Braden):

After _n_ such encounters the posterior density for $\theta$ is

$$
\pi(\theta|X_1< y_1,\dots,X_n<y_n) \varpropto \pi(\theta) \prod_{i=1}^n\int_{-\infty}^{y_i}
   \exp\left(-{(x-\theta)^2\over{2\sigma^2}}\right)\ dx
$$

Another equation:

$$\det\left|\,\begin{matrix}%
c_0&c_1\hfill&c_2\hfill&\ldots&c_n\hfill\cr
c_1&c_2\hfill&c_3\hfill&\ldots&c_{n+1}\hfill\cr
c_2&c_3\hfill&c_4\hfill&\ldots&c_{n+2}\hfill\cr
\,\vdots\hfill&\,\vdots\hfill&
  \,\vdots\hfill&&\,\vdots\hfill\cr
c_n&c_{n+1}\hfill&c_{n+2}\hfill&\ldots&c_{2n}\hfill\cr
\end{matrix}\right|>0$$


Lapidus and Pindar, Numerical Solution of Partial Differential Equations in Science and
Engineering.  Page 54

$$
\int_t\left\{\sum_{j=1}^3 T_j \left({d\phi_j\over dt}+k\phi_j\right)-kT_e\right\}w_i(t)\ dt=0,
   \qquad\quad i=1,2,3.
$$

L\&P  Galerkin method weighting functions.  Page 55

$$
\sum_{j=1}^3 T_j\int_0^1\left\{{d\phi_j\over dt} + k\phi_j\right\} \phi_i\ dt
   = \int_{0}^1k\,T_e\phi_idt, \qquad i=1,2,3 $$

Another L\&P (p145)

$$
\int_{-1}^1\!\int_{-1}^1\!\int_{-1}^1 f\big(\xi,\eta,\zeta\big)
   = \sum_{k=1}^n\sum_{j=1}^n\sum_{i=1}^n w_i w_j w_k f\big( \xi,\eta,\zeta\big).
$$

Another L\&P (p126)

$$
\int_{A_e} (\,\cdot\,) dx dy = \int_{-1}^1\!\int_{-1}^1 (\,\cdot\,) \det[J] d\xi d\eta.
$$
-->

## Chemistry 101: Symbols

Chemical formulas will look best if they are not italicized. Get around math mode's automatic italicizing in LaTeX by using the argument `$\mathrm{formula here}$`, with your formula inside the curly brackets.  (Notice the use of the backticks here which enclose text that acts as code.)

So, $\mathrm{Fe_2^{2+}Cr_2O_4}$ is written `$\mathrm{Fe_2^{2+}Cr_2O_4}$`.

<!--
The \noindent command below does what you'd expect:  it forces the current line/paragraph to not indent. This was done here to match the format of the LaTeX thesis PDF.
-->

\noindent Exponent or Superscript: $\mathrm{O^-}$

\noindent Subscript: $\mathrm{CH_4}$

To stack numbers or letters as in $\mathrm{Fe_2^{2+}}$, the subscript is defined first, and then the superscript is defined.

\noindent Bullet: CuCl $\bullet$ $\mathrm{7H_{2}O}$


\noindent Delta: $\Delta$

\noindent Reaction Arrows: $\longrightarrow$ or  $\xrightarrow{solution}$

\noindent Resonance Arrows: $\leftrightarrow$

\noindent Reversible Reaction Arrows: $\rightleftharpoons$

### Typesetting reactions

You may wish to put your reaction in an equation environment, which means that LaTeX will place the reaction where it fits and will number the equations for you. 

\begin{equation}
  \mathrm{C_6H_{12}O_6  + 6O_2} \longrightarrow \mathrm{6CO_2 + 6H_2O}
  (\#eq:reaction)
\end{equation}

We can reference this combustion of glucose reaction via Equation \@ref(eq:reaction).

### Other examples of reactions

$\mathrm{NH_4Cl_{(s)}}$ $\rightleftharpoons$ $\mathrm{NH_{3(g)}+HCl_{(g)}}$

\noindent $\mathrm{MeCH_2Br + Mg}$ $\xrightarrow[below]{above}$ $\mathrm{MeCH_2\bullet Mg \bullet Br}$

## Physics

Many of the symbols you will need can be found on the math page <https://web.reed.edu/cis/help/latex/math.html> and the Comprehensive LaTeX Symbol Guide (<https://mirror.utexas.edu/ctan/info/symbols/comprehensive/symbols-letter.pdf>).

## Biology

You will probably find the resources at <https://www.lecb.ncifcrf.gov/~toms/latex.html> helpful, particularly the links to bsts for various journals. You may also be interested in TeXShade for nucleotide typesetting (<https://homepages.uni-tuebingen.de/beitz/txe.html>).  Be sure to read the proceeding chapter on graphics and tables.

